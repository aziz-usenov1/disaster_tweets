{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<center>\n",
    "<h1> Natural Language Processing with Disaster texts </h1>\n",
    "<h2>Data Mining and Big Data Analytics</h2>\n",
    "<h3>Azizbek Ussenov</h3>\n",
    "<h4>2024-2025</h4>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/azizbek.ussenov/anaconda3/envs/mining/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "import gensim.downloader as api\n",
    "import re\n",
    "import html\n",
    "import emoji\n",
    "import contractions\n",
    "import pandas as pd\n",
    "import torch\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Dataset\n",
    "df_train = pd.read_csv('train.csv')\n",
    "\n",
    "# Test Dataset\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    4342\n",
       "1    3271\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is class imbalance non-disaster tweets being dominant. In general, transformer will be used as a final model. As transformers do not need much preprocessing, I will not deal with class imbalance. It is because when I tried to handle them the performance got decreased, so in my final model there will be less preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = df_test[['id']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df, columns:list):\n",
    "    return df.drop(columns=columns, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = drop_columns(df_train, ['id','keyword', 'location'])\n",
    "df_test = drop_columns(df_test, ['id','keyword', 'location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1\n",
       "1                Forest fire near La Ronge Sask. Canada       1\n",
       "2     All residents asked to 'shelter in place' are ...       1\n",
       "3     13,000 people receive #wildfires evacuation or...       1\n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1\n",
       "...                                                 ...     ...\n",
       "7608  Two giant cranes holding a bridge collapse int...       1\n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1\n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1\n",
       "7611  Police investigating after an e-bike collided ...       1\n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1\n",
       "\n",
       "[7613 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I remove everything that looks messy and lemmatize i.e. groups similar words, reducing the number of different words in a data set and simplifying text data. This preprocessing is solely for the models with One-hot encoding, Bag of Words, TF-IFD, and Word2vec. I will preprocess the tweets with less processing once again for transformer as I approach towards end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def clean_preprocess(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.strip()\n",
    "    text = html.unescape(text)                              # Fix &amp;, &lt;, etc.\n",
    "\n",
    "    text = text.replace(\"‰ÛÏ\", '\"').replace(\"‰Û\", '\"')     # double quotes\n",
    "    text = text.replace(\"‰Û÷\", \"'\").replace(\"‰Ûª\", \"'\")     # single quotes\n",
    "    text = text.replace(\"‰Û¢\", \"-\").replace(\"‰Û_\", \"-\")     # hyphen issues\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
    "    text = re.sub(r\"#\", \"\", text)                           # remove hashtags\n",
    "    text = re.sub(r\"\\n\", \" \", text)                         # remove newline chars\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)                     # remove URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)                        # remove mentions\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))      # convert emoji to text\n",
    "\n",
    "    text = re.sub(r\"w/e\", \"whatever\", text)                 # typos, slang and informal abbreviations below\n",
    "    text = re.sub(r\"w/\", \"with\", text)\n",
    "    text = re.sub(r\"Ph0tos\", \"Photos\", text)\n",
    "    text = re.sub(r\"amirite\", \"am I right\", text)\n",
    "    text = re.sub(r\"exp0sed\", \"exposed\", text)\n",
    "    text = re.sub(r\"<3\", \"love\", text)\n",
    "    text = re.sub(r\"amageddon\", \"armageddon\", text)\n",
    "    text = re.sub(r\"Trfc\", \"Traffic\", text)\n",
    "    text = re.sub(r\"8/5/2015\", \"2015-08-05\", text)\n",
    "    text = re.sub(r\"WindStorm\", \"Wind Storm\", text)\n",
    "    text = re.sub(r\"8/6/2015\", \"2015-08-06\", text)\n",
    "    text = re.sub(r\"10:38PM\", \"10:38 PM\", text)\n",
    "    text = re.sub(r\"10:30pm\", \"10:30 PM\", text)\n",
    "    text = re.sub(r\"16yr\", \"16 year\", text)\n",
    "    text = re.sub(r\"lmao\", \"laughing my ass off\", text)   \n",
    "\n",
    "\n",
    "    text = re.sub(r\"PantherAttack\", \"Panther Attack\", text)  # hashtags and usernames below\n",
    "    text = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", text)\n",
    "    text = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", text)\n",
    "    text = re.sub(r\"thankU\", \"thank you\", text)\n",
    "    text = re.sub(r\"OffensiveContent\", \"Offensive Content\", text)\n",
    "    text = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", text)\n",
    "    text = re.sub(r\"aRmageddon\", \"armageddon\", text)\n",
    "    text = re.sub(r\"Throwingknifes\", \"Throwing knives\", text)\n",
    "    text = re.sub(r\"til_now\", \"until now\", text)\n",
    "    text = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", text)\n",
    "    text = re.sub(r\"abstorm\", \"Alberta Storm\", text)\n",
    "    text = re.sub(r\"IDFire\", \"Idaho Fire\", text)\n",
    "    text = re.sub(r\"DETECTADO\", \"Detected\", text)\n",
    "    text = re.sub(r\"RockyFire\", \"Rocky Fire\", text)\n",
    "    text = re.sub(r\"yycstorm\", \"Calgary Storm\", text)\n",
    "    text = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", text)\n",
    "    text = re.sub(r\"NoSurrender\", \"No Surrender\", text)\n",
    "    text = re.sub(r\"NotExplained\", \"Not Explained\", text)\n",
    "    text = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", text)\n",
    "    text = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", text)\n",
    "    text = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", text)\n",
    "    text = re.sub(r\"SouthDowns\", \"South Downs\", text)\n",
    "    text = re.sub(r\"S3XLEAK\", \"sex leak\", text)\n",
    "\n",
    "\n",
    "    text = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", text)       # urls\n",
    "\n",
    "\n",
    "    text = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", text)   # acronyms below\n",
    "    text = re.sub(r\"mÌ¼sica\", \"music\", text)\n",
    "    text = re.sub(r\"okwx\", \"Oklahoma City Weather\", text)\n",
    "    text = re.sub(r\"arwx\", \"Arkansas Weather\", text)    \n",
    "    text = re.sub(r\"gawx\", \"Georgia Weather\", text)  \n",
    "    text = re.sub(r\"scwx\", \"South Carolina Weather\", text)  \n",
    "    text = re.sub(r\"cawx\", \"California Weather\", text)\n",
    "    text = re.sub(r\"tnwx\", \"Tennessee Weather\", text)\n",
    "    text = re.sub(r\"azwx\", \"Arizona Weather\", text)  \n",
    "    text = re.sub(r\"alwx\", \"Alabama Weather\", text)\n",
    "    text = re.sub(r\"wordpressdotcom\", \"wordpress\", text)    \n",
    "    text = re.sub(r\"usNWSgov\", \"United States National Weather Service\", text)\n",
    "    text = re.sub(r\"Suruc\", \"Sanliurfa\", text)   \n",
    "    \n",
    "    text = contractions.fix(text)    # expand contractions\n",
    "    \n",
    "    text = text.lower()\n",
    "\n",
    "    doc = nlp(text) # lemmatization\n",
    "    lemmatized_text = \" \".join([token.lemma_ for token in doc if not token.is_punct and not token.is_space])\n",
    "    \n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"text\"] = df_train[\"text\"].apply(clean_preprocess)\n",
    "df_test[\"text\"] = df_test[\"text\"].apply(clean_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>our deed be the reason of this earthquake may ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all resident ask to shelter in place be be not...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive wildfire evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>just get send this photo from ruby alaska as s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>two giant crane hold a bridge collapse into ne...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>the out of control wild fire in california eve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>m1.94 01:04 utc]?5 km s of volcano hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>police investigate after an e bike collide wit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>the late more home raze by northern california...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "0     our deed be the reason of this earthquake may ...       1\n",
       "1                 forest fire near la ronge sask canada       1\n",
       "2     all resident ask to shelter in place be be not...       1\n",
       "3     13,000 people receive wildfire evacuation orde...       1\n",
       "4     just get send this photo from ruby alaska as s...       1\n",
       "...                                                 ...     ...\n",
       "7608  two giant crane hold a bridge collapse into ne...       1\n",
       "7609  the out of control wild fire in california eve...       1\n",
       "7610          m1.94 01:04 utc]?5 km s of volcano hawaii       1\n",
       "7611  police investigate after an e bike collide wit...       1\n",
       "7612  the late more home raze by northern california...       1\n",
       "\n",
       "[7613 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some duplicates with different target values. So, let's inspect them. Afterwards, I will drop them by keeping the first one as the first occurences best labels the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conflicting = df_train.groupby(\"text\")[\"target\"].nunique()\n",
    "conflicting_texts = conflicting[conflicting > 1].index\n",
    "\n",
    "conflicting_rows = df_train[df_train[\"text\"].isin(conflicting_texts)]\n",
    "conflicting_rows = conflicting_rows.sort_values(\"text\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.potus strategicpatience be a strategy for gen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.potus strategicpatience be a strategy for gen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.potus strategicpatience be a strategy for gen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.potus strategicpatience be a strategy for gen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.potus strategicpatience be a strategy for gen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>you.s national park service tonto national for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>you.s national park service tonto national for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>you.s national park service tonto national for...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>you.s national park service tonto national for...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>you.s national park service tonto national for...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>287 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  target\n",
       "0    .potus strategicpatience be a strategy for gen...       1\n",
       "1    .potus strategicpatience be a strategy for gen...       1\n",
       "2    .potus strategicpatience be a strategy for gen...       1\n",
       "3    .potus strategicpatience be a strategy for gen...       0\n",
       "4    .potus strategicpatience be a strategy for gen...       1\n",
       "..                                                 ...     ...\n",
       "282  you.s national park service tonto national for...       1\n",
       "283  you.s national park service tonto national for...       1\n",
       "284  you.s national park service tonto national for...       0\n",
       "285  you.s national park service tonto national for...       0\n",
       "286  you.s national park service tonto national for...       0\n",
       "\n",
       "[287 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conflicting_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0:\n",
      ".potus strategicpatience be a strategy for genocide refugee idp internally displace people horror etc\n",
      "1\n",
      "----------------------------------------\n",
      "Row 1:\n",
      ".potus strategicpatience be a strategy for genocide refugee idp internally displace people horror etc\n",
      "1\n",
      "----------------------------------------\n",
      "Row 2:\n",
      ".potus strategicpatience be a strategy for genocide refugee idp internally displace people horror etc\n",
      "1\n",
      "----------------------------------------\n",
      "Row 3:\n",
      ".potus strategicpatience be a strategy for genocide refugee idp internally displace people horror etc\n",
      "0\n",
      "----------------------------------------\n",
      "Row 4:\n",
      ".potus strategicpatience be a strategy for genocide refugee idp internally displace people horror etc\n",
      "1\n",
      "----------------------------------------\n",
      "Row 5:\n",
      "2pc 18w cree lead work light offroad lamp car truck boat mining 4wd flood beam full rea\n",
      "0\n",
      "----------------------------------------\n",
      "Row 6:\n",
      "2pc 18w cree lead work light offroad lamp car truck boat mining 4wd flood beam full rea\n",
      "1\n",
      "----------------------------------------\n",
      "Row 7:\n",
      "2pc 18w cree lead work light offroad lamp car truck boat mining 4wd flood beam full rea\n",
      "0\n",
      "----------------------------------------\n",
      "Row 8:\n",
      "2pc 18w cree lead work light offroad lamp car truck boat mining 4wd flood beam full rea\n",
      "0\n",
      "----------------------------------------\n",
      "Row 9:\n",
      "360wisenews china 's stock market crash be there gem in the rubble\n",
      "1\n",
      "----------------------------------------\n",
      "Row 10:\n",
      "360wisenews china 's stock market crash be there gem in the rubble\n",
      "0\n",
      "----------------------------------------\n",
      "Row 11:\n",
      "360wisenews china 's stock market crash be there gem in the rubble\n",
      "0\n",
      "----------------------------------------\n",
      "Row 12:\n",
      "360wisenews china 's stock market crash be there gem in the rubble\n",
      "1\n",
      "----------------------------------------\n",
      "Row 13:\n",
      "I can not drown my demon they know how to swim\n",
      "0\n",
      "----------------------------------------\n",
      "Row 14:\n",
      "I can not drown my demon they know how to swim\n",
      "1\n",
      "----------------------------------------\n",
      "Row 15:\n",
      "I like a video from gun range mayhem\n",
      "1\n",
      "----------------------------------------\n",
      "Row 16:\n",
      "I like a video from gun range mayhem\n",
      "0\n",
      "----------------------------------------\n",
      "Row 17:\n",
      "I pledge allegiance to the p.o.p.e and the burn building of epic city\n",
      "0\n",
      "----------------------------------------\n",
      "Row 18:\n",
      "I pledge allegiance to the p.o.p.e and the burn building of epic city\n",
      "1\n",
      "----------------------------------------\n",
      "Row 19:\n",
      "a look at state action a year after ferguson 's upheaval\n",
      "1\n",
      "----------------------------------------\n",
      "Row 20:\n",
      "a look at state action a year after ferguson 's upheaval\n",
      "0\n",
      "----------------------------------------\n",
      "Row 21:\n",
      "a look at state action a year after ferguson 's upheaval\n",
      "1\n",
      "----------------------------------------\n",
      "Row 22:\n",
      "a look at state action a year after ferguson 's upheaval\n",
      "0\n",
      "----------------------------------------\n",
      "Row 23:\n",
      "allah describe pile up wealth think it would last forever as the description of the people of hellfire in surah humaza reflect\n",
      "1\n",
      "----------------------------------------\n",
      "Row 24:\n",
      "allah describe pile up wealth think it would last forever as the description of the people of hellfire in surah humaza reflect\n",
      "0\n",
      "----------------------------------------\n",
      "Row 25:\n",
      "allah describe pile up wealth think it would last forever as the description of the people of hellfire in surah humaza reflect\n",
      "0\n",
      "----------------------------------------\n",
      "Row 26:\n",
      "angry woman openly accuse nema of steal relief material mean for idps an angry internally displace wom\n",
      "0\n",
      "----------------------------------------\n",
      "Row 27:\n",
      "angry woman openly accuse nema of steal relief material mean for idps an angry internally displace wom\n",
      "1\n",
      "----------------------------------------\n",
      "Row 28:\n",
      "angry woman openly accuse nema of steal relief material mean for idps an angry internally displace wom\n",
      "0\n",
      "----------------------------------------\n",
      "Row 29:\n",
      "armageddon\n",
      "0\n",
      "----------------------------------------\n",
      "Row 30:\n",
      "armageddon\n",
      "1\n",
      "----------------------------------------\n",
      "Row 31:\n",
      "ashe 2015 australia collapse at trent bridge among bad in history england bundle out australia for 60\n",
      "0\n",
      "----------------------------------------\n",
      "Row 32:\n",
      "ashe 2015 australia collapse at trent bridge among bad in history england bundle out australia for 60\n",
      "1\n",
      "----------------------------------------\n",
      "Row 33:\n",
      "bayelsa poll tension in bayelsa as patience jonathan plan to hijack apc pdp plan by former first lady and\n",
      "1\n",
      "----------------------------------------\n",
      "Row 34:\n",
      "bayelsa poll tension in bayelsa as patience jonathan plan to hijack apc pdp plan by former first lady and\n",
      "0\n",
      "----------------------------------------\n",
      "Row 35:\n",
      "bayelsa poll tension in bayelsa as patience jonathan plan to hijack apc pdp plan by former first lady and\n",
      "0\n",
      "----------------------------------------\n",
      "Row 36:\n",
      "caution breathing may be hazardous to your health\n",
      "0\n",
      "----------------------------------------\n",
      "Row 37:\n",
      "caution breathing may be hazardous to your health\n",
      "1\n",
      "----------------------------------------\n",
      "Row 38:\n",
      "china stock market crash this summer have spark interest from bargain hunt\n",
      "1\n",
      "----------------------------------------\n",
      "Row 39:\n",
      "china stock market crash this summer have spark interest from bargain hunt\n",
      "0\n",
      "----------------------------------------\n",
      "Row 40:\n",
      "choke hazard prompt recall of kraft cheese single\n",
      "0\n",
      "----------------------------------------\n",
      "Row 41:\n",
      "choke hazard prompt recall of kraft cheese single\n",
      "1\n",
      "----------------------------------------\n",
      "Row 42:\n",
      "cindy noonan heartbreak in baltimore rioting yahistorical undergroundrailraod\n",
      "0\n",
      "----------------------------------------\n",
      "Row 43:\n",
      "cindy noonan heartbreak in baltimore rioting yahistorical undergroundrailraod\n",
      "1\n",
      "----------------------------------------\n",
      "Row 44:\n",
      "clear incident with injury i-495 inner loop exit 31 md 97 georgia ave silver spring\n",
      "0\n",
      "----------------------------------------\n",
      "Row 45:\n",
      "clear incident with injury i-495 inner loop exit 31 md 97 georgia ave silver spring\n",
      "1\n",
      "----------------------------------------\n",
      "Row 46:\n",
      "clear incident with injury i-495 inner loop exit 31 md 97 georgia ave silver spring\n",
      "1\n",
      "----------------------------------------\n",
      "Row 47:\n",
      "detonate feat m.o.p by apollo brown\n",
      "0\n",
      "----------------------------------------\n",
      "Row 48:\n",
      "detonate feat m.o.p by apollo brown\n",
      "1\n",
      "----------------------------------------\n",
      "Row 49:\n",
      "detonation fashionable mountaineer electronic watch water resistant couple leisure tab\n",
      "0\n",
      "----------------------------------------\n",
      "Row 50:\n",
      "detonation fashionable mountaineer electronic watch water resistant couple leisure tab\n",
      "0\n",
      "----------------------------------------\n",
      "Row 51:\n",
      "detonation fashionable mountaineer electronic watch water resistant couple leisure tab\n",
      "1\n",
      "----------------------------------------\n",
      "Row 52:\n",
      "do you feel like you be sink in low self image take the quiz\n",
      "1\n",
      "----------------------------------------\n",
      "Row 53:\n",
      "do you feel like you be sink in low self image take the quiz\n",
      "0\n",
      "----------------------------------------\n",
      "Row 54:\n",
      "drunk meal 101 what to cook when you be totally obliterate\n",
      "0\n",
      "----------------------------------------\n",
      "Row 55:\n",
      "drunk meal 101 what to cook when you be totally obliterate\n",
      "1\n",
      "----------------------------------------\n",
      "Row 56:\n",
      "drunk meal 101 what to cook when you be totally obliterate\n",
      "0\n",
      "----------------------------------------\n",
      "Row 57:\n",
      "drunk meal 101 what to cook when you be totally obliterate\n",
      "0\n",
      "----------------------------------------\n",
      "Row 58:\n",
      "earthquake drill\n",
      "0\n",
      "----------------------------------------\n",
      "Row 59:\n",
      "earthquake drill\n",
      "1\n",
      "----------------------------------------\n",
      "Row 60:\n",
      "enugu government to demolish illegal structure at international conference centre\n",
      "0\n",
      "----------------------------------------\n",
      "Row 61:\n",
      "enugu government to demolish illegal structure at international conference centre\n",
      "0\n",
      "----------------------------------------\n",
      "Row 62:\n",
      "enugu government to demolish illegal structure at international conference centre\n",
      "1\n",
      "----------------------------------------\n",
      "Row 63:\n",
      "fatality\n",
      "0\n",
      "----------------------------------------\n",
      "Row 64:\n",
      "fatality\n",
      "0\n",
      "----------------------------------------\n",
      "Row 65:\n",
      "fatality\n",
      "1\n",
      "----------------------------------------\n",
      "Row 66:\n",
      "fatality\n",
      "0\n",
      "----------------------------------------\n",
      "Row 67:\n",
      "fatality\n",
      "0\n",
      "----------------------------------------\n",
      "Row 68:\n",
      "fatality\n",
      "0\n",
      "----------------------------------------\n",
      "Row 69:\n",
      "fatality\n",
      "0\n",
      "----------------------------------------\n",
      "Row 70:\n",
      "fatality\n",
      "0\n",
      "----------------------------------------\n",
      "Row 71:\n",
      "fedex no long to transport bioterror germ in wake of anthrax lab mishap\n",
      "1\n",
      "----------------------------------------\n",
      "Row 72:\n",
      "fedex no long to transport bioterror germ in wake of anthrax lab mishap\n",
      "1\n",
      "----------------------------------------\n",
      "Row 73:\n",
      "fedex no long to transport bioterror germ in wake of anthrax lab mishap\n",
      "0\n",
      "----------------------------------------\n",
      "Row 74:\n",
      "fedex no long to transport bioterror germ in wake of anthrax lab mishap\n",
      "1\n",
      "----------------------------------------\n",
      "Row 75:\n",
      "fedex no long to transport bioterror germ in wake of anthrax lab mishap via\n",
      "1\n",
      "----------------------------------------\n",
      "Row 76:\n",
      "fedex no long to transport bioterror germ in wake of anthrax lab mishap via\n",
      "1\n",
      "----------------------------------------\n",
      "Row 77:\n",
      "fedex no long to transport bioterror germ in wake of anthrax lab mishap via\n",
      "1\n",
      "----------------------------------------\n",
      "Row 78:\n",
      "fedex no long to transport bioterror germ in wake of anthrax lab mishap via\n",
      "0\n",
      "----------------------------------------\n",
      "Row 79:\n",
      "fettilootch be slanglucci oppression great danger come soon the album\n",
      "0\n",
      "----------------------------------------\n",
      "Row 80:\n",
      "fettilootch be slanglucci oppression great danger come soon the album\n",
      "0\n",
      "----------------------------------------\n",
      "Row 81:\n",
      "fettilootch be slanglucci oppression great danger come soon the album\n",
      "0\n",
      "----------------------------------------\n",
      "Row 82:\n",
      "fettilootch be slanglucci oppression great danger come soon the album\n",
      "1\n",
      "----------------------------------------\n",
      "Row 83:\n",
      "fire hazard associate with installation of non compliant external cladding on by .cbplawyers\n",
      "0\n",
      "----------------------------------------\n",
      "Row 84:\n",
      "fire hazard associate with installation of non compliant external cladding on by .cbplawyers\n",
      "1\n",
      "----------------------------------------\n",
      "Row 85:\n",
      "foodscare offers2go nestleindia slip into loss after magginoodle ban unsafe and hazardous for humanconsumption\n",
      "1\n",
      "----------------------------------------\n",
      "Row 86:\n",
      "foodscare offers2go nestleindia slip into loss after magginoodle ban unsafe and hazardous for humanconsumption\n",
      "1\n",
      "----------------------------------------\n",
      "Row 87:\n",
      "foodscare offers2go nestleindia slip into loss after magginoodle ban unsafe and hazardous for humanconsumption\n",
      "0\n",
      "----------------------------------------\n",
      "Row 88:\n",
      "gm I pray any attack of the enemy 2 derail you be destiny be block by the lord that he flood you be life withheavenly blessing\n",
      "0\n",
      "----------------------------------------\n",
      "Row 89:\n",
      "gm I pray any attack of the enemy 2 derail you be destiny be block by the lord that he flood you be life withheavenly blessing\n",
      "0\n",
      "----------------------------------------\n",
      "Row 90:\n",
      "gm I pray any attack of the enemy 2 derail you be destiny be block by the lord that he flood you be life withheavenly blessing\n",
      "0\n",
      "----------------------------------------\n",
      "Row 91:\n",
      "gm I pray any attack of the enemy 2 derail you be destiny be block by the lord that he flood you be life withheavenly blessing\n",
      "1\n",
      "----------------------------------------\n",
      "Row 92:\n",
      "gm I pray any attack of the enemy 2 derail you be destiny be block by the lord that he flood you be life withheavenly blessing\n",
      "0\n",
      "----------------------------------------\n",
      "Row 93:\n",
      "gm I pray any attack of the enemy 2 derail you be destiny be block by the lord that he flood you be life withheavenly blessing\n",
      "0\n",
      "----------------------------------------\n",
      "Row 94:\n",
      "governor weigh parole for california school bus hijacker\n",
      "1\n",
      "----------------------------------------\n",
      "Row 95:\n",
      "governor weigh parole for california school bus hijacker\n",
      "0\n",
      "----------------------------------------\n",
      "Row 96:\n",
      "he come to a land which be engulf in tribal war and turn it into a land of peace i.e. madinah prophetmuhammad islam\n",
      "0\n",
      "----------------------------------------\n",
      "Row 97:\n",
      "he come to a land which be engulf in tribal war and turn it into a land of peace i.e. madinah prophetmuhammad islam\n",
      "0\n",
      "----------------------------------------\n",
      "Row 98:\n",
      "he come to a land which be engulf in tribal war and turn it into a land of peace i.e. madinah prophetmuhammad islam\n",
      "1\n",
      "----------------------------------------\n",
      "Row 99:\n",
      "he come to a land which be engulf in tribal war and turn it into a land of peace i.e. madinah prophetmuhammad islam\n",
      "1\n",
      "----------------------------------------\n",
      "Row 100:\n",
      "he come to a land which be engulf in tribal war and turn it into a land of peace i.e. madinah prophetmuhammad islam\n",
      "0\n",
      "----------------------------------------\n",
      "Row 101:\n",
      "he come to a land which be engulf in tribal war and turn it into a land of peace i.e. madinah prophetmuhammad islam\n",
      "0\n",
      "----------------------------------------\n",
      "Row 102:\n",
      "hellfire be surround by desire so be careful and do not let your desire control you afterlife\n",
      "0\n",
      "----------------------------------------\n",
      "Row 103:\n",
      "hellfire be surround by desire so be careful and do not let your desire control you afterlife\n",
      "0\n",
      "----------------------------------------\n",
      "Row 104:\n",
      "hellfire be surround by desire so be careful and do not let your desire control you afterlife\n",
      "1\n",
      "----------------------------------------\n",
      "Row 105:\n",
      "hellfire we do not even want to think about it or mention it so let we not do anything that lead to it islam\n",
      "0\n",
      "----------------------------------------\n",
      "Row 106:\n",
      "hellfire we do not even want to think about it or mention it so let we not do anything that lead to it islam\n",
      "1\n",
      "----------------------------------------\n",
      "Row 107:\n",
      "here be how medium in pakistan cover the capture of terrorist mohamme nave\n",
      "1\n",
      "----------------------------------------\n",
      "Row 108:\n",
      "here be how medium in pakistan cover the capture of terrorist mohamme nave\n",
      "0\n",
      "----------------------------------------\n",
      "Row 109:\n",
      "hollywood movie about trap miner release in chile\n",
      "0\n",
      "----------------------------------------\n",
      "Row 110:\n",
      "hollywood movie about trap miner release in chile\n",
      "0\n",
      "----------------------------------------\n",
      "Row 111:\n",
      "hollywood movie about trap miner release in chile\n",
      "1\n",
      "----------------------------------------\n",
      "Row 112:\n",
      "hollywood movie about trap miner release in chile\n",
      "0\n",
      "----------------------------------------\n",
      "Row 113:\n",
      "hollywood movie about trap miner release in chile\n",
      "0\n",
      "----------------------------------------\n",
      "Row 114:\n",
      "hollywood movie about trap miner release in chile\n",
      "0\n",
      "----------------------------------------\n",
      "Row 115:\n",
      "hollywood movie about trap miner release in chile the 33 hollywood movie about trap miner star\n",
      "0\n",
      "----------------------------------------\n",
      "Row 116:\n",
      "hollywood movie about trap miner release in chile the 33 hollywood movie about trap miner star\n",
      "1\n",
      "----------------------------------------\n",
      "Row 117:\n",
      "hollywood movie about trap miner release in chile the 33 hollywood movie about trap miner star\n",
      "1\n",
      "----------------------------------------\n",
      "Row 118:\n",
      "hollywood movie about trap miner release in chile the 33 hollywood movie about trap miner star\n",
      "0\n",
      "----------------------------------------\n",
      "Row 119:\n",
      "hollywood movie about trap miner release in chile the 33 hollywood movie about trap miner star\n",
      "0\n",
      "----------------------------------------\n",
      "Row 120:\n",
      "hot c-130 specially modify to land in a stadium and rescue hostage in iran in 1980 prebreak well\n",
      "0\n",
      "----------------------------------------\n",
      "Row 121:\n",
      "hot c-130 specially modify to land in a stadium and rescue hostage in iran in 1980 prebreak well\n",
      "1\n",
      "----------------------------------------\n",
      "Row 122:\n",
      "hot c-130 specially modify to land in a stadium and rescue hostage in iran in 1980 prebreak well\n",
      "1\n",
      "----------------------------------------\n",
      "Row 123:\n",
      "hot c-130 specially modify to land in a stadium and rescue hostage in iran in 1980 prebreak well\n",
      "1\n",
      "----------------------------------------\n",
      "Row 124:\n",
      "hot c-130 specially modify to land in a stadium and rescue hostage in iran in 1980 prebreak well\n",
      "1\n",
      "----------------------------------------\n",
      "Row 125:\n",
      "hot c-130 specially modify to land in a stadium and rescue hostage in iran in 1980 prebreak well\n",
      "1\n",
      "----------------------------------------\n",
      "Row 126:\n",
      "hot c-130 specially modify to land in a stadium and rescue hostage in iran in 1980 prebreak well\n",
      "1\n",
      "----------------------------------------\n",
      "Row 127:\n",
      "hot c-130 specially modify to land in a stadium and rescue hostage in iran in 1980 prebreak well\n",
      "1\n",
      "----------------------------------------\n",
      "Row 128:\n",
      "hot funtenna hijack computer to send datum as sound wave black hat 2015 prebreak well\n",
      "1\n",
      "----------------------------------------\n",
      "Row 129:\n",
      "hot funtenna hijack computer to send datum as sound wave black hat 2015 prebreak well\n",
      "0\n",
      "----------------------------------------\n",
      "Row 130:\n",
      "hot funtenna hijack computer to send datum as sound wave black hat 2015 prebreak well\n",
      "0\n",
      "----------------------------------------\n",
      "Row 131:\n",
      "hot funtenna hijack computer to send datum as sound wave black hat 2015 prebreak well\n",
      "0\n",
      "----------------------------------------\n",
      "Row 132:\n",
      "hot funtenna hijack computer to send datum as sound wave black hat 2015 prebreak well\n",
      "0\n",
      "----------------------------------------\n",
      "Row 133:\n",
      "hot funtenna hijack computer to send datum as sound wave black hat 2015 prebreak well\n",
      "0\n",
      "----------------------------------------\n",
      "Row 134:\n",
      "hot funtenna hijack computer to send datum as sound wave black hat 2015 prebreak well\n",
      "0\n",
      "----------------------------------------\n",
      "Row 135:\n",
      "hot funtenna hijack computer to send datum as sound wave black hat 2015 prebreak well\n",
      "1\n",
      "----------------------------------------\n",
      "Row 136:\n",
      "hot funtenna hijack computer to send datum as sound wave black hat 2015 prebreak well\n",
      "0\n",
      "----------------------------------------\n",
      "Row 137:\n",
      "hot funtenna hijack computer to send datum as sound wave black hat 2015 prebreak well\n",
      "0\n",
      "----------------------------------------\n",
      "Row 138:\n",
      "hot funtenna hijack computer to send datum as sound wave black hat 2015 prebreak well\n",
      "0\n",
      "----------------------------------------\n",
      "Row 139:\n",
      "hot funtenna hijack computer to send datum as sound wave black hat 2015 prebreak well\n",
      "0\n",
      "----------------------------------------\n",
      "Row 140:\n",
      "hot funtenna hijack computer to send datum as sound wave black hat 2015 prebreak well\n",
      "0\n",
      "----------------------------------------\n",
      "Row 141:\n",
      "hot funtenna hijack computer to send datum as sound wave black hat 2015 prebreak well\n",
      "1\n",
      "----------------------------------------\n",
      "Row 142:\n",
      "hot funtenna hijack computer to send datum as sound wave black hat 2015 prebreak well\n",
      "0\n",
      "----------------------------------------\n",
      "Row 143:\n",
      "hot funtenna hijack computer to send datum as sound wave black hat 2015 prebreak well\n",
      "1\n",
      "----------------------------------------\n",
      "Row 144:\n",
      "hot funtenna hijack computer to send datum as sound wave black hat 2015 prebreak well\n",
      "1\n",
      "----------------------------------------\n",
      "Row 145:\n",
      "hot reddit 's new content policy go into effect many horrible subreddit ban or quarantine prebreak well\n",
      "0\n",
      "----------------------------------------\n",
      "Row 146:\n",
      "hot reddit 's new content policy go into effect many horrible subreddit ban or quarantine prebreak well\n",
      "1\n",
      "----------------------------------------\n",
      "Row 147:\n",
      "hot reddit 's new content policy go into effect many horrible subreddit ban or quarantine prebreak well\n",
      "0\n",
      "----------------------------------------\n",
      "Row 148:\n",
      "hot reddit 's new content policy go into effect many horrible subreddit ban or quarantine prebreak well\n",
      "0\n",
      "----------------------------------------\n",
      "Row 149:\n",
      "hot reddit 's new content policy go into effect many horrible subreddit ban or quarantine prebreak well\n",
      "0\n",
      "----------------------------------------\n",
      "Row 150:\n",
      "how be it one careless match can start a forest fire but it take a whole box to start a campfire\n",
      "0\n",
      "----------------------------------------\n",
      "Row 151:\n",
      "how be it one careless match can start a forest fire but it take a whole box to start a campfire\n",
      "1\n",
      "----------------------------------------\n",
      "Row 152:\n",
      "in islam save a person be equal in reward to save all human islam be the opposite of terrorism\n",
      "0\n",
      "----------------------------------------\n",
      "Row 153:\n",
      "in islam save a person be equal in reward to save all human islam be the opposite of terrorism\n",
      "1\n",
      "----------------------------------------\n",
      "Row 154:\n",
      "kosciusko police investigate pedestrian fatality hit by a train thursday\n",
      "0\n",
      "----------------------------------------\n",
      "Row 155:\n",
      "kosciusko police investigate pedestrian fatality hit by a train thursday\n",
      "1\n",
      "----------------------------------------\n",
      "Row 156:\n",
      "like for the music video I want some real action shit like burn building and police chase not some weak ben winston shit\n",
      "0\n",
      "----------------------------------------\n",
      "Row 157:\n",
      "like for the music video I want some real action shit like burn building and police chase not some weak ben winston shit\n",
      "1\n",
      "----------------------------------------\n",
      "Row 158:\n",
      "love what you pick we be play worth it by fifth harm kid ink because of you listen vote\n",
      "0\n",
      "----------------------------------------\n",
      "Row 159:\n",
      "love what you pick we be play worth it by fifth harm kid ink because of you listen vote\n",
      "1\n",
      "----------------------------------------\n",
      "Row 160:\n",
      "love what you pick we be play worth it by fifth harm kid ink because of you listen vote\n",
      "0\n",
      "----------------------------------------\n",
      "Row 161:\n",
      "mmmmmm I be burn I be burn building I be build oooooohhhh oooh ooh\n",
      "0\n",
      "----------------------------------------\n",
      "Row 162:\n",
      "mmmmmm I be burn I be burn building I be build oooooohhhh oooh ooh\n",
      "1\n",
      "----------------------------------------\n",
      "Row 163:\n",
      "new lady shoulder tote handbag faux leather hobo purse cross body bag women\n",
      "0\n",
      "----------------------------------------\n",
      "Row 164:\n",
      "new lady shoulder tote handbag faux leather hobo purse cross body bag women\n",
      "0\n",
      "----------------------------------------\n",
      "Row 165:\n",
      "new lady shoulder tote handbag faux leather hobo purse cross body bag women\n",
      "1\n",
      "----------------------------------------\n",
      "Row 166:\n",
      "new lady shoulder tote handbag faux leather hobo purse cross body bag women\n",
      "0\n",
      "----------------------------------------\n",
      "Row 167:\n",
      "pandemonium in aba as woman deliver baby without face photo\n",
      "0\n",
      "----------------------------------------\n",
      "Row 168:\n",
      "pandemonium in aba as woman deliver baby without face photo\n",
      "0\n",
      "----------------------------------------\n",
      "Row 169:\n",
      "pandemonium in aba as woman deliver baby without face photo\n",
      "0\n",
      "----------------------------------------\n",
      "Row 170:\n",
      "pandemonium in aba as woman deliver baby without face photo\n",
      "1\n",
      "----------------------------------------\n",
      "Row 171:\n",
      "pandemonium in aba as woman deliver baby without face photo\n",
      "0\n",
      "----------------------------------------\n",
      "Row 172:\n",
      "pandemonium in aba as woman deliver baby without face photo\n",
      "0\n",
      "----------------------------------------\n",
      "Row 173:\n",
      "pandemonium in aba as woman deliver baby without face photo\n",
      "1\n",
      "----------------------------------------\n",
      "Row 174:\n",
      "pandemonium in aba as woman deliver baby without face photo\n",
      "1\n",
      "----------------------------------------\n",
      "Row 175:\n",
      "pandemonium in aba as woman deliver baby without face photo\n",
      "0\n",
      "----------------------------------------\n",
      "Row 176:\n",
      "pandemonium in aba as woman deliver baby without face photo\n",
      "0\n",
      "----------------------------------------\n",
      "Row 177:\n",
      "petition | heartless owner that whip horse until it collapse be tell he can keep his animal act now\n",
      "0\n",
      "----------------------------------------\n",
      "Row 178:\n",
      "petition | heartless owner that whip horse until it collapse be tell he can keep his animal act now\n",
      "0\n",
      "----------------------------------------\n",
      "Row 179:\n",
      "petition | heartless owner that whip horse until it collapse be tell he can keep his animal act now\n",
      "1\n",
      "----------------------------------------\n",
      "Row 180:\n",
      "reddit 's new content policy go into effect many horrible subreddit ban or quarantine\n",
      "0\n",
      "----------------------------------------\n",
      "Row 181:\n",
      "reddit 's new content policy go into effect many horrible subreddit ban or quarantine\n",
      "0\n",
      "----------------------------------------\n",
      "Row 182:\n",
      "reddit 's new content policy go into effect many horrible subreddit ban or quarantine\n",
      "0\n",
      "----------------------------------------\n",
      "Row 183:\n",
      "reddit 's new content policy go into effect many horrible subreddit ban or quarantine\n",
      "0\n",
      "----------------------------------------\n",
      "Row 184:\n",
      "reddit 's new content policy go into effect many horrible subreddit ban or quarantine\n",
      "0\n",
      "----------------------------------------\n",
      "Row 185:\n",
      "reddit 's new content policy go into effect many horrible subreddit ban or quarantine\n",
      "1\n",
      "----------------------------------------\n",
      "Row 186:\n",
      "reddit update content policy promise to quarantine extremely offensive community\n",
      "1\n",
      "----------------------------------------\n",
      "Row 187:\n",
      "reddit update content policy promise to quarantine extremely offensive community\n",
      "0\n",
      "----------------------------------------\n",
      "Row 188:\n",
      "reddit update content policy promise to quarantine extremely offensive community\n",
      "0\n",
      "----------------------------------------\n",
      "Row 189:\n",
      "reddit update content policy promise to quarantine extremely offensive community\n",
      "0\n",
      "----------------------------------------\n",
      "Row 190:\n",
      "reddit will now quarantine offensive content\n",
      "1\n",
      "----------------------------------------\n",
      "Row 191:\n",
      "reddit will now quarantine offensive content\n",
      "0\n",
      "----------------------------------------\n",
      "Row 192:\n",
      "reddit will now quarantine offensive content\n",
      "1\n",
      "----------------------------------------\n",
      "Row 193:\n",
      "reddit will now quarantine offensive content\n",
      "0\n",
      "----------------------------------------\n",
      "Row 194:\n",
      "reddit will now quarantine offensive content\n",
      "0\n",
      "----------------------------------------\n",
      "Row 195:\n",
      "reddit will now quarantine offensive content\n",
      "1\n",
      "----------------------------------------\n",
      "Row 196:\n",
      "reddit will now quarantine offensive content\n",
      "0\n",
      "----------------------------------------\n",
      "Row 197:\n",
      "reddit will now quarantine offensive content\n",
      "0\n",
      "----------------------------------------\n",
      "Row 198:\n",
      "reddit will now quarantine offensive content\n",
      "0\n",
      "----------------------------------------\n",
      "Row 199:\n",
      "reddit will now quarantine offensive content\n",
      "0\n",
      "----------------------------------------\n",
      "Row 200:\n",
      "remove the and linkury browser hijacker\n",
      "0\n",
      "----------------------------------------\n",
      "Row 201:\n",
      "remove the and linkury browser hijacker\n",
      "1\n",
      "----------------------------------------\n",
      "Row 202:\n",
      "remove the and linkury browser hijacker\n",
      "0\n",
      "----------------------------------------\n",
      "Row 203:\n",
      "remove the and linkury browser hijacker\n",
      "0\n",
      "----------------------------------------\n",
      "Row 204:\n",
      "remove the and linkury browser hijacker\n",
      "0\n",
      "----------------------------------------\n",
      "Row 205:\n",
      "remove the and linkury browser hijacker\n",
      "0\n",
      "----------------------------------------\n",
      "Row 206:\n",
      "remove the and linkury browser hijacker\n",
      "1\n",
      "----------------------------------------\n",
      "Row 207:\n",
      "rt not explain the only know image of infamous hijacker d.b cooper\n",
      "1\n",
      "----------------------------------------\n",
      "Row 208:\n",
      "rt not explain the only know image of infamous hijacker d.b cooper\n",
      "0\n",
      "----------------------------------------\n",
      "Row 209:\n",
      "sassy city girl country hunk strand in smoky mountain snowstorm aom ibooklove bookboost\n",
      "1\n",
      "----------------------------------------\n",
      "Row 210:\n",
      "sassy city girl country hunk strand in smoky mountain snowstorm aom ibooklove bookboost\n",
      "0\n",
      "----------------------------------------\n",
      "Row 211:\n",
      "sassy city girl country hunk strand in smoky mountain snowstorm aom ibooklove bookboost\n",
      "1\n",
      "----------------------------------------\n",
      "Row 212:\n",
      "spot flood combo 53inch 300w curved cree lead work light bar 4x4 offroad fog lamp full re\n",
      "0\n",
      "----------------------------------------\n",
      "Row 213:\n",
      "spot flood combo 53inch 300w curved cree lead work light bar 4x4 offroad fog lamp full re\n",
      "1\n",
      "----------------------------------------\n",
      "Row 214:\n",
      "spot flood combo 53inch 300w curved cree lead work light bar 4x4 offroad fog lamp full re\n",
      "0\n",
      "----------------------------------------\n",
      "Row 215:\n",
      "spot flood combo 53inch 300w curved cree lead work light bar 4x4 offroad fog lamp full re\n",
      "0\n",
      "----------------------------------------\n",
      "Row 216:\n",
      "spot flood combo 53inch 300w curved cree lead work light bar 4x4 offroad fog lamp full re\n",
      "0\n",
      "----------------------------------------\n",
      "Row 217:\n",
      "spot flood combo 53inch 300w curved cree lead work light bar 4x4 offroad fog lamp full re\n",
      "0\n",
      "----------------------------------------\n",
      "Row 218:\n",
      "spot flood combo 53inch 300w curved cree lead work light bar 4x4 offroad fog lamp full re\n",
      "0\n",
      "----------------------------------------\n",
      "Row 219:\n",
      "star war power of the jedi collection 1 battle droid hasbro full read by ebay\n",
      "0\n",
      "----------------------------------------\n",
      "Row 220:\n",
      "star war power of the jedi collection 1 battle droid hasbro full read by ebay\n",
      "1\n",
      "----------------------------------------\n",
      "Row 221:\n",
      "survival kit whistle fire starter wire see cree torch emergency blanket s knife full re\n",
      "0\n",
      "----------------------------------------\n",
      "Row 222:\n",
      "survival kit whistle fire starter wire see cree torch emergency blanket s knife full re\n",
      "1\n",
      "----------------------------------------\n",
      "Row 223:\n",
      "swansea plot hijack transfer move for southampton target virgil van dijk\n",
      "1\n",
      "----------------------------------------\n",
      "Row 224:\n",
      "swansea plot hijack transfer move for southampton target virgil van dijk\n",
      "1\n",
      "----------------------------------------\n",
      "Row 225:\n",
      "swansea plot hijack transfer move for southampton target virgil van dijk\n",
      "0\n",
      "----------------------------------------\n",
      "Row 226:\n",
      "that horrible sink feeling when you have be at home on your phone for a while and you realise its be on 3 g this whole time\n",
      "1\n",
      "----------------------------------------\n",
      "Row 227:\n",
      "that horrible sink feeling when you have be at home on your phone for a while and you realise its be on 3 g this whole time\n",
      "1\n",
      "----------------------------------------\n",
      "Row 228:\n",
      "that horrible sink feeling when you have be at home on your phone for a while and you realise its be on 3 g this whole time\n",
      "0\n",
      "----------------------------------------\n",
      "Row 229:\n",
      "that horrible sink feeling when you have be at home on your phone for a while and you realise its be on 3 g this whole time\n",
      "0\n",
      "----------------------------------------\n",
      "Row 230:\n",
      "that horrible sink feeling when you have be at home on your phone for a while and you realise its be on 3 g this whole time\n",
      "0\n",
      "----------------------------------------\n",
      "Row 231:\n",
      "that horrible sink feeling when you have be at home on your phone for a while and you realise its be on 3 g this whole time\n",
      "0\n",
      "----------------------------------------\n",
      "Row 232:\n",
      "that horrible sink feeling when you have be at home on your phone for a while and you realise its be on 3 g this whole time\n",
      "1\n",
      "----------------------------------------\n",
      "Row 233:\n",
      "the dress meme have officially explode on the internet\n",
      "0\n",
      "----------------------------------------\n",
      "Row 234:\n",
      "the dress meme have officially explode on the internet\n",
      "0\n",
      "----------------------------------------\n",
      "Row 235:\n",
      "the dress meme have officially explode on the internet\n",
      "1\n",
      "----------------------------------------\n",
      "Row 236:\n",
      "the prophet peace be upon he say save yourself from hellfire even if it be by give half a date in charity\n",
      "0\n",
      "----------------------------------------\n",
      "Row 237:\n",
      "the prophet peace be upon he say save yourself from hellfire even if it be by give half a date in charity\n",
      "1\n",
      "----------------------------------------\n",
      "Row 238:\n",
      "the prophet peace be upon he say save yourself from hellfire even if it be by give half a date in charity\n",
      "0\n",
      "----------------------------------------\n",
      "Row 239:\n",
      "the prophet peace be upon he say save yourself from hellfire even if it be by give half a date in charity\n",
      "1\n",
      "----------------------------------------\n",
      "Row 240:\n",
      "the prophet peace be upon he say save yourself from hellfire even if it be by give half a date in charity\n",
      "0\n",
      "----------------------------------------\n",
      "Row 241:\n",
      "the prophet peace be upon he say save yourself from hellfire even if it be by give half a date in charity\n",
      "0\n",
      "----------------------------------------\n",
      "Row 242:\n",
      "the way you move be like a full on rainstorm and I be a house of card\n",
      "1\n",
      "----------------------------------------\n",
      "Row 243:\n",
      "the way you move be like a full on rainstorm and I be a house of card\n",
      "0\n",
      "----------------------------------------\n",
      "Row 244:\n",
      "to fight bioterrorism sir\n",
      "1\n",
      "----------------------------------------\n",
      "Row 245:\n",
      "to fight bioterrorism sir\n",
      "0\n",
      "----------------------------------------\n",
      "Row 246:\n",
      "to fight bioterrorism sir\n",
      "1\n",
      "----------------------------------------\n",
      "Row 247:\n",
      "to fight bioterrorism sir\n",
      "0\n",
      "----------------------------------------\n",
      "Row 248:\n",
      "trafford centre film fan angry after odeon cinema evacuate follow false fire alarm\n",
      "1\n",
      "----------------------------------------\n",
      "Row 249:\n",
      "trafford centre film fan angry after odeon cinema evacuate follow false fire alarm\n",
      "0\n",
      "----------------------------------------\n",
      "Row 250:\n",
      "truth news bbc cnn islam truth god isis terrorism quran lie\n",
      "0\n",
      "----------------------------------------\n",
      "Row 251:\n",
      "truth news bbc cnn islam truth god isis terrorism quran lie\n",
      "1\n",
      "----------------------------------------\n",
      "Row 252:\n",
      "truth news bbc cnn islam truth god isis terrorism quran lie\n",
      "1\n",
      "----------------------------------------\n",
      "Row 253:\n",
      "truth news bbc cnn islam truth god isis terrorism quran lie\n",
      "0\n",
      "----------------------------------------\n",
      "Row 254:\n",
      "truth news bbc cnn islam truth god isis terrorism quran lie\n",
      "1\n",
      "----------------------------------------\n",
      "Row 255:\n",
      "truth news bbc cnn islam truth god isis terrorism quran lie\n",
      "0\n",
      "----------------------------------------\n",
      "Row 256:\n",
      "truth news bbc cnn islam truth god isis terrorism quran lie\n",
      "1\n",
      "----------------------------------------\n",
      "Row 257:\n",
      "twia board approve 5 percent rate hike the texas windstorm insurance association twia board of director v\n",
      "1\n",
      "----------------------------------------\n",
      "Row 258:\n",
      "twia board approve 5 percent rate hike the texas windstorm insurance association twia board of director v\n",
      "0\n",
      "----------------------------------------\n",
      "Row 259:\n",
      "ty for the follow go to brutally abused+desolate&lost + her lovely mum die be it murder\n",
      "0\n",
      "----------------------------------------\n",
      "Row 260:\n",
      "ty for the follow go to brutally abused+desolate&lost + her lovely mum die be it murder\n",
      "1\n",
      "----------------------------------------\n",
      "Row 261:\n",
      "watch sarah palin obliterate plan parenthood for target minority woman bb4sp\n",
      "0\n",
      "----------------------------------------\n",
      "Row 262:\n",
      "watch sarah palin obliterate plan parenthood for target minority woman bb4sp\n",
      "0\n",
      "----------------------------------------\n",
      "Row 263:\n",
      "watch sarah palin obliterate plan parenthood for target minority woman bb4sp\n",
      "1\n",
      "----------------------------------------\n",
      "Row 264:\n",
      "we need help horse will die!please rt sign petition!take a stand be a voice for they gilbert23\n",
      "1\n",
      "----------------------------------------\n",
      "Row 265:\n",
      "we need help horse will die!please rt sign petition!take a stand be a voice for they gilbert23\n",
      "0\n",
      "----------------------------------------\n",
      "Row 266:\n",
      "who be bring the tornado and flood who be bring the climate change god be after america he be plague she farrakhan quote\n",
      "1\n",
      "----------------------------------------\n",
      "Row 267:\n",
      "who be bring the tornado and flood who be bring the climate change god be after america he be plague she farrakhan quote\n",
      "0\n",
      "----------------------------------------\n",
      "Row 268:\n",
      "who be bring the tornado and flood who be bring the climate change god be after america he be plague she farrakhan quote\n",
      "0\n",
      "----------------------------------------\n",
      "Row 269:\n",
      "why be you deluge with low self image take the quiz\n",
      "0\n",
      "----------------------------------------\n",
      "Row 270:\n",
      "why be you deluge with low self image take the quiz\n",
      "1\n",
      "----------------------------------------\n",
      "Row 271:\n",
      "world annihilation vs self transformation alien attack to exterminate human\n",
      "0\n",
      "----------------------------------------\n",
      "Row 272:\n",
      "world annihilation vs self transformation alien attack to exterminate human\n",
      "1\n",
      "----------------------------------------\n",
      "Row 273:\n",
      "world fedex no long to transport bioterror germ in wake of anthrax lab mishap\n",
      "1\n",
      "----------------------------------------\n",
      "Row 274:\n",
      "world fedex no long to transport bioterror germ in wake of anthrax lab mishap\n",
      "0\n",
      "----------------------------------------\n",
      "Row 275:\n",
      "world war ii book lightning joe an autobiography by general j. lawton collins\n",
      "1\n",
      "----------------------------------------\n",
      "Row 276:\n",
      "world war ii book lightning joe an autobiography by general j. lawton collins\n",
      "0\n",
      "----------------------------------------\n",
      "Row 277:\n",
      "wowo--=== 12000 nigerian refugee repatriate from cameroon\n",
      "1\n",
      "----------------------------------------\n",
      "Row 278:\n",
      "wowo--=== 12000 nigerian refugee repatriate from cameroon\n",
      "0\n",
      "----------------------------------------\n",
      "Row 279:\n",
      "you.s national park service tonto national forest stop the annihilation of the salt river wild horse via\n",
      "0\n",
      "----------------------------------------\n",
      "Row 280:\n",
      "you.s national park service tonto national forest stop the annihilation of the salt river wild horse via\n",
      "0\n",
      "----------------------------------------\n",
      "Row 281:\n",
      "you.s national park service tonto national forest stop the annihilation of the salt river wild horse via\n",
      "1\n",
      "----------------------------------------\n",
      "Row 282:\n",
      "you.s national park service tonto national forest stop the annihilation of the salt river wild horse via\n",
      "1\n",
      "----------------------------------------\n",
      "Row 283:\n",
      "you.s national park service tonto national forest stop the annihilation of the salt river wild horse via\n",
      "1\n",
      "----------------------------------------\n",
      "Row 284:\n",
      "you.s national park service tonto national forest stop the annihilation of the salt river wild horse via\n",
      "0\n",
      "----------------------------------------\n",
      "Row 285:\n",
      "you.s national park service tonto national forest stop the annihilation of the salt river wild horse via\n",
      "0\n",
      "----------------------------------------\n",
      "Row 286:\n",
      "you.s national park service tonto national forest stop the annihilation of the salt river wild horse via\n",
      "0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for idx, row in conflicting_rows.iterrows():\n",
    "    print(f\"Row {idx}:\")\n",
    "    print(row['text'])\n",
    "    print(row['target'])\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before drop duplicates: 7613\n",
      "after drop duplicates: 6858\n"
     ]
    }
   ],
   "source": [
    "print(\"before drop duplicates:\", len(df_train))\n",
    "\n",
    "df_train.drop_duplicates(subset=['text'], keep='first', inplace=True)\n",
    "\n",
    "print(\"after drop duplicates:\", len(df_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the distribution of text lengths in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_text_lengths = [len(row) for row in df_train.text]\n",
    "\n",
    "mean_len = np.mean(train_text_lengths)\n",
    "median_len = np.median(train_text_lengths)\n",
    "max_len = max(train_text_lengths)\n",
    "min_len = min(train_text_lengths)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.hist(train_text_lengths, bins=40, color='brown', edgecolor='black', alpha=0.7)\n",
    "\n",
    "plt.axvline(mean_len, color='purple', linestyle='dashed', linewidth=2, label=f'Mean: {mean_len:.1f}')\n",
    "plt.axvline(median_len, color='green', linestyle='dashed', linewidth=2, label=f'Median: {median_len:.0f}')\n",
    "plt.axvline(max_len, color='black', linestyle='dashed', linewidth=2, label=f'Max: {max_len}')\n",
    "plt.axvline(min_len, color='orange', linestyle='dashed', linewidth=2, label=f'Min: {min_len}')\n",
    "\n",
    "plt.title('Distribution of Text Lengths in Training Data')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:01:09,833] A new study created in memory with name: no-name-4d6b3afb-1f42-4769-966a-23915acd2d23\n",
      "[I 2025-04-06 22:01:12,536] Trial 0 finished with value: 0.6972533461413528 and parameters: {'binary': False, 'ngram_range': (1, 2), 'min_df': 1, 'max_features': 3000, 'C': 0.017008919244797294}. Best is trial 0 with value: 0.6972533461413528.\n",
      "[I 2025-04-06 22:01:13,175] Trial 1 finished with value: 0.6732022501398199 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 5, 'max_features': 5000, 'C': 2.2525749178918844}. Best is trial 0 with value: 0.6972533461413528.\n",
      "[I 2025-04-06 22:01:13,309] Trial 2 finished with value: 0.6890452662893125 and parameters: {'binary': False, 'ngram_range': (1, 2), 'min_df': 3, 'max_features': None, 'C': 0.24982405867658836}. Best is trial 0 with value: 0.6972533461413528.\n",
      "[I 2025-04-06 22:01:13,889] Trial 3 finished with value: 0.6992755082043441 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 1, 'max_features': None, 'C': 0.5087342291820904}. Best is trial 3 with value: 0.6992755082043441.\n",
      "[I 2025-04-06 22:01:14,526] Trial 4 finished with value: 0.6986912907273068 and parameters: {'binary': False, 'ngram_range': (1, 2), 'min_df': 4, 'max_features': 5000, 'C': 0.019452799078187255}. Best is trial 3 with value: 0.6992755082043441.\n",
      "[I 2025-04-06 22:01:14,663] Trial 5 finished with value: 0.6904787563339999 and parameters: {'binary': False, 'ngram_range': (1, 2), 'min_df': 1, 'max_features': 3000, 'C': 0.16782251760599998}. Best is trial 3 with value: 0.6992755082043441.\n",
      "[I 2025-04-06 22:01:14,724] Trial 6 finished with value: 0.7015611187784204 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 1, 'max_features': 1000, 'C': 0.15454364603688203}. Best is trial 6 with value: 0.7015611187784204.\n",
      "[I 2025-04-06 22:01:14,782] Trial 7 finished with value: 0.7070259127460797 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 5, 'max_features': 1000, 'C': 0.09345019201190069}. Best is trial 7 with value: 0.7070259127460797.\n",
      "[I 2025-04-06 22:01:14,905] Trial 8 finished with value: 0.7061558007190403 and parameters: {'binary': False, 'ngram_range': (1, 2), 'min_df': 1, 'max_features': 5000, 'C': 0.04682971433197747}. Best is trial 7 with value: 0.7070259127460797.\n",
      "[I 2025-04-06 22:01:15,016] Trial 9 finished with value: 0.7047017538958059 and parameters: {'binary': False, 'ngram_range': (1, 2), 'min_df': 1, 'max_features': 5000, 'C': 0.02961347564177737}. Best is trial 7 with value: 0.7070259127460797.\n",
      "[I 2025-04-06 22:01:15,112] Trial 10 finished with value: 0.6700541724098529 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 5, 'max_features': 1000, 'C': 5.614845249457767}. Best is trial 7 with value: 0.7070259127460797.\n",
      "[I 2025-04-06 22:01:15,180] Trial 11 finished with value: 0.7091590746415939 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 3, 'max_features': 1000, 'C': 0.060653602391935714}. Best is trial 11 with value: 0.7091590746415939.\n",
      "[I 2025-04-06 22:01:15,728] Trial 12 finished with value: 0.7086458510096726 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 3, 'max_features': 1000, 'C': 0.06989627853054908}. Best is trial 11 with value: 0.7091590746415939.\n",
      "[I 2025-04-06 22:01:15,797] Trial 13 finished with value: 0.6912233735005454 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 3, 'max_features': 1000, 'C': 0.7123240261876618}. Best is trial 11 with value: 0.7091590746415939.\n",
      "[I 2025-04-06 22:01:15,853] Trial 14 finished with value: 0.7117093786225333 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 2, 'max_features': 1000, 'C': 0.060020814166040035}. Best is trial 14 with value: 0.7117093786225333.\n",
      "[I 2025-04-06 22:01:15,907] Trial 15 finished with value: 0.6880372739689323 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 2, 'max_features': 1000, 'C': 0.012434750457545725}. Best is trial 14 with value: 0.7117093786225333.\n",
      "[I 2025-04-06 22:01:15,974] Trial 16 finished with value: 0.7120675117848262 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 2, 'max_features': 1000, 'C': 0.046211215557067684}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:16,042] Trial 17 finished with value: 0.6843691210651114 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 2, 'max_features': 1000, 'C': 1.219805949970914}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:16,108] Trial 18 finished with value: 0.7044919713943608 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 2, 'max_features': None, 'C': 0.03382740584213123}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:16,164] Trial 19 finished with value: 0.7058161968335289 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 2, 'max_features': 1000, 'C': 0.12150413531310524}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:16,232] Trial 20 finished with value: 0.6851965292986786 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 4, 'max_features': 3000, 'C': 0.38876597142105984}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:16,285] Trial 21 finished with value: 0.7114118240012064 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 2, 'max_features': 1000, 'C': 0.05708912837065116}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:16,338] Trial 22 finished with value: 0.7053411613646933 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 2, 'max_features': 1000, 'C': 0.030303055670451787}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:16,394] Trial 23 finished with value: 0.7116557337390049 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 2, 'max_features': 1000, 'C': 0.06269077529505261}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:16,448] Trial 24 finished with value: 0.6866860404981115 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 2, 'max_features': 1000, 'C': 0.011486782610997828}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:16,514] Trial 25 finished with value: 0.7039919504165192 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 4, 'max_features': 1000, 'C': 0.09526623071524411}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:16,582] Trial 26 finished with value: 0.7014171618173237 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 2, 'max_features': 1000, 'C': 0.19775477978594158}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:16,648] Trial 27 finished with value: 0.7024954527113513 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 3, 'max_features': 3000, 'C': 0.02313762821311035}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:16,716] Trial 28 finished with value: 0.7081089024377909 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 2, 'max_features': None, 'C': 0.04224302307910249}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:16,890] Trial 29 finished with value: 0.6986554828194516 and parameters: {'binary': False, 'ngram_range': (1, 2), 'min_df': 1, 'max_features': 3000, 'C': 0.01831864645948104}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:16,956] Trial 30 finished with value: 0.6962726311573214 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 3, 'max_features': 1000, 'C': 0.3147091966417754}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:17,023] Trial 31 finished with value: 0.7105830587501867 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 2, 'max_features': 1000, 'C': 0.07469063738940485}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:17,078] Trial 32 finished with value: 0.7117093786225333 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 2, 'max_features': 1000, 'C': 0.05977513993594577}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:17,135] Trial 33 finished with value: 0.7052989646977768 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 2, 'max_features': 1000, 'C': 0.10796671526713171}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:17,189] Trial 34 finished with value: 0.7092606874822635 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 1, 'max_features': 1000, 'C': 0.044555727258783887}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:17,257] Trial 35 finished with value: 0.6945692422667245 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 3, 'max_features': 5000, 'C': 0.216302562138343}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:17,358] Trial 36 finished with value: 0.7053942068348378 and parameters: {'binary': False, 'ngram_range': (1, 2), 'min_df': 2, 'max_features': None, 'C': 0.023841112252082866}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:17,434] Trial 37 finished with value: 0.6682978443780163 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 3, 'max_features': 1000, 'C': 9.408472618216662}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:17,547] Trial 38 finished with value: 0.6977838333577798 and parameters: {'binary': False, 'ngram_range': (1, 2), 'min_df': 1, 'max_features': 1000, 'C': 0.1424858157432231}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:17,600] Trial 39 finished with value: 0.7041746066611466 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 1, 'max_features': 5000, 'C': 0.015268570542030721}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:17,663] Trial 40 finished with value: 0.6826261653732345 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 2, 'max_features': 3000, 'C': 0.5787579972573864}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:17,718] Trial 41 finished with value: 0.7116592099337197 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 2, 'max_features': 1000, 'C': 0.05437570062012757}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:17,773] Trial 42 finished with value: 0.7093907220240894 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 2, 'max_features': 1000, 'C': 0.07791917051039496}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:17,827] Trial 43 finished with value: 0.7091963836764177 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 2, 'max_features': 1000, 'C': 0.03888012866196468}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:17,895] Trial 44 finished with value: 0.7093192490131708 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 3, 'max_features': 1000, 'C': 0.061554511820713816}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:18,002] Trial 45 finished with value: 0.6971437321904433 and parameters: {'binary': False, 'ngram_range': (1, 2), 'min_df': 1, 'max_features': 1000, 'C': 0.1512983401989237}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:18,058] Trial 46 finished with value: 0.7094952914265473 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 2, 'max_features': None, 'C': 0.04842405333108761}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:18,114] Trial 47 finished with value: 0.7045277034287635 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 1, 'max_features': 5000, 'C': 0.02472652836863427}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:18,168] Trial 48 finished with value: 0.7044764925676131 and parameters: {'binary': False, 'ngram_range': (1, 1), 'min_df': 3, 'max_features': 1000, 'C': 0.10150698025728168}. Best is trial 16 with value: 0.7120675117848262.\n",
      "[I 2025-04-06 22:01:18,286] Trial 49 finished with value: 0.6886714532669055 and parameters: {'binary': False, 'ngram_range': (1, 2), 'min_df': 2, 'max_features': 1000, 'C': 0.26597073645099734}. Best is trial 16 with value: 0.7120675117848262.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Hyperparameters Found:\n",
      "binary: False\n",
      "ngram_range: (1, 1)\n",
      "min_df: 2\n",
      "max_features: 1000\n",
      "C: 0.046211215557067684\n",
      "\n",
      " Validation Results:\n",
      "F1 Score: 0.7141403865717192\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.84       838\n",
      "           1       0.78      0.66      0.71       534\n",
      "\n",
      "    accuracy                           0.80      1372\n",
      "   macro avg       0.79      0.77      0.78      1372\n",
      "weighted avg       0.79      0.80      0.79      1372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = df_train[\"text\"]\n",
    "y = df_train[\"target\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    binary = trial.suggest_categorical(\"binary\", [False])\n",
    "    ngram_range = trial.suggest_categorical(\"ngram_range\", [(1, 1), (1, 2)])\n",
    "    min_df = trial.suggest_int(\"min_df\", 1, 5)\n",
    "    max_features = trial.suggest_categorical(\"max_features\", [None, 1000, 3000, 5000])\n",
    "\n",
    "    \n",
    "    C = trial.suggest_float(\"C\", 0.01, 10.0, log=True) # model hyperparameter\n",
    "\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"vectorizer\", CountVectorizer(\n",
    "            binary=binary,\n",
    "            ngram_range=ngram_range,\n",
    "            min_df=min_df,\n",
    "            max_features=max_features\n",
    "        )),\n",
    "        (\"svm\", LinearSVC(C=C, max_iter=10000))\n",
    "    ])\n",
    "\n",
    "    score = cross_val_score(pipeline, X_train, y_train, scoring=\"f1\", cv=3, n_jobs=-1).mean()\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\" Best Hyperparameters Found:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# best param pipeline\n",
    "best_pipeline = Pipeline([\n",
    "    (\"vectorizer\", CountVectorizer(\n",
    "        binary=best_params[\"binary\"],\n",
    "        ngram_range=best_params[\"ngram_range\"],\n",
    "        min_df=best_params[\"min_df\"],\n",
    "        max_features=best_params[\"max_features\"]\n",
    "    )),\n",
    "    (\"svm\", LinearSVC(C=best_params[\"C\"], max_iter=10000))\n",
    "])\n",
    "\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "y_pred = best_pipeline.predict(X_val)\n",
    "\n",
    "print(\"\\n Validation Results:\")\n",
    "print(\"F1 Score:\", f1_score(y_val, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = best_pipeline.predict(df_test['text'])\n",
    "\n",
    "# Add predictions to test_df\n",
    "df_test[\"target\"] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission_2 = pd.DataFrame()\n",
    "# submission_2['id'] = test_id['id']\n",
    "# submission_2['target'] = test_preds\n",
    "# # submission_2.to_csv(\"submission_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:01:18,391] A new study created in memory with name: no-name-6dc12eb5-1a29-479a-8949-d530f01ada4f\n",
      "[I 2025-04-06 22:01:18,468] Trial 0 finished with value: 0.6749305815484883 and parameters: {'binary': False, 'max_features': None, 'ngram_range': (1, 1), 'min_df': 5, 'C': 1.8339294021410915}. Best is trial 0 with value: 0.6749305815484883.\n",
      "[I 2025-04-06 22:01:18,520] Trial 1 finished with value: 0.6986989417144992 and parameters: {'binary': False, 'max_features': 3000, 'ngram_range': (1, 1), 'min_df': 3, 'C': 0.15026535170494246}. Best is trial 1 with value: 0.6986989417144992.\n",
      "[I 2025-04-06 22:01:18,573] Trial 2 finished with value: 0.7056496628185557 and parameters: {'binary': False, 'max_features': 5000, 'ngram_range': (1, 1), 'min_df': 4, 'C': 0.034561621668044634}. Best is trial 2 with value: 0.7056496628185557.\n",
      "[I 2025-04-06 22:01:18,685] Trial 3 finished with value: 0.6863166692893365 and parameters: {'binary': False, 'max_features': 5000, 'ngram_range': (1, 2), 'min_df': 5, 'C': 0.16368840817592625}. Best is trial 2 with value: 0.7056496628185557.\n",
      "[I 2025-04-06 22:01:18,744] Trial 4 finished with value: 0.6588637340190499 and parameters: {'binary': False, 'max_features': None, 'ngram_range': (1, 1), 'min_df': 4, 'C': 3.4077438850723083}. Best is trial 2 with value: 0.7056496628185557.\n",
      "[I 2025-04-06 22:01:18,866] Trial 5 finished with value: 0.6922540494552969 and parameters: {'binary': False, 'max_features': 5000, 'ngram_range': (1, 2), 'min_df': 1, 'C': 0.22023291196012876}. Best is trial 2 with value: 0.7056496628185557.\n",
      "[I 2025-04-06 22:01:18,981] Trial 6 finished with value: 0.6772869730219537 and parameters: {'binary': False, 'max_features': 1000, 'ngram_range': (1, 2), 'min_df': 5, 'C': 0.8190558482359312}. Best is trial 2 with value: 0.7056496628185557.\n",
      "[I 2025-04-06 22:01:19,108] Trial 7 finished with value: 0.6937180475933425 and parameters: {'binary': False, 'max_features': 5000, 'ngram_range': (1, 2), 'min_df': 2, 'C': 0.20666079957336256}. Best is trial 2 with value: 0.7056496628185557.\n",
      "[I 2025-04-06 22:01:19,242] Trial 8 finished with value: 0.6926967628077509 and parameters: {'binary': False, 'max_features': None, 'ngram_range': (1, 2), 'min_df': 2, 'C': 0.3368714622021972}. Best is trial 2 with value: 0.7056496628185557.\n",
      "[I 2025-04-06 22:01:19,615] Trial 9 finished with value: 0.647857153489971 and parameters: {'binary': False, 'max_features': 3000, 'ngram_range': (1, 2), 'min_df': 1, 'C': 7.186070567336969}. Best is trial 2 with value: 0.7056496628185557.\n",
      "[I 2025-04-06 22:01:19,682] Trial 10 finished with value: 0.6938645408591504 and parameters: {'binary': False, 'max_features': 1000, 'ngram_range': (1, 1), 'min_df': 4, 'C': 0.01750149285874934}. Best is trial 2 with value: 0.7056496628185557.\n",
      "[I 2025-04-06 22:01:19,750] Trial 11 finished with value: 0.7031694883318235 and parameters: {'binary': False, 'max_features': 3000, 'ngram_range': (1, 1), 'min_df': 3, 'C': 0.027332397757226557}. Best is trial 2 with value: 0.7056496628185557.\n",
      "[I 2025-04-06 22:01:19,817] Trial 12 finished with value: 0.7050348835280978 and parameters: {'binary': False, 'max_features': 3000, 'ngram_range': (1, 1), 'min_df': 3, 'C': 0.016993363445940032}. Best is trial 2 with value: 0.7056496628185557.\n",
      "[I 2025-04-06 22:01:19,875] Trial 13 finished with value: 0.7109984712034837 and parameters: {'binary': False, 'max_features': 5000, 'ngram_range': (1, 1), 'min_df': 4, 'C': 0.04021567741638942}. Best is trial 13 with value: 0.7109984712034837.\n",
      "[I 2025-04-06 22:01:19,941] Trial 14 finished with value: 0.711051969185874 and parameters: {'binary': False, 'max_features': 5000, 'ngram_range': (1, 1), 'min_df': 4, 'C': 0.0424305920934791}. Best is trial 14 with value: 0.711051969185874.\n",
      "[I 2025-04-06 22:01:20,011] Trial 15 finished with value: 0.7120004614720807 and parameters: {'binary': False, 'max_features': 5000, 'ngram_range': (1, 1), 'min_df': 4, 'C': 0.06042771894031737}. Best is trial 15 with value: 0.7120004614720807.\n",
      "[I 2025-04-06 22:01:20,080] Trial 16 finished with value: 0.7102062078346666 and parameters: {'binary': False, 'max_features': 5000, 'ngram_range': (1, 1), 'min_df': 4, 'C': 0.0712997252911564}. Best is trial 15 with value: 0.7120004614720807.\n",
      "[I 2025-04-06 22:01:20,145] Trial 17 finished with value: 0.693459897812755 and parameters: {'binary': False, 'max_features': 5000, 'ngram_range': (1, 1), 'min_df': 2, 'C': 0.010522481528961802}. Best is trial 15 with value: 0.7120004614720807.\n",
      "[I 2025-04-06 22:01:20,235] Trial 18 finished with value: 0.704829971767006 and parameters: {'binary': False, 'max_features': 5000, 'ngram_range': (1, 1), 'min_df': 3, 'C': 0.0782373130735469}. Best is trial 15 with value: 0.7120004614720807.\n",
      "[I 2025-04-06 22:01:20,337] Trial 19 finished with value: 0.6915608957201927 and parameters: {'binary': False, 'max_features': 1000, 'ngram_range': (1, 1), 'min_df': 5, 'C': 0.5390850179470762}. Best is trial 15 with value: 0.7120004614720807.\n",
      "[I 2025-04-06 22:01:20,405] Trial 20 finished with value: 0.7095622182946918 and parameters: {'binary': False, 'max_features': 5000, 'ngram_range': (1, 1), 'min_df': 4, 'C': 0.07378365300261201}. Best is trial 15 with value: 0.7120004614720807.\n",
      "[I 2025-04-06 22:01:20,472] Trial 21 finished with value: 0.7129523527707503 and parameters: {'binary': False, 'max_features': 5000, 'ngram_range': (1, 1), 'min_df': 4, 'C': 0.04468022025147841}. Best is trial 21 with value: 0.7129523527707503.\n",
      "[I 2025-04-06 22:01:20,540] Trial 22 finished with value: 0.7125120551645953 and parameters: {'binary': False, 'max_features': 5000, 'ngram_range': (1, 1), 'min_df': 4, 'C': 0.05712486929697599}. Best is trial 21 with value: 0.7129523527707503.\n",
      "[I 2025-04-06 22:01:20,605] Trial 23 finished with value: 0.7095017123485934 and parameters: {'binary': False, 'max_features': 5000, 'ngram_range': (1, 1), 'min_df': 5, 'C': 0.08885814722390885}. Best is trial 21 with value: 0.7129523527707503.\n",
      "[I 2025-04-06 22:01:20,693] Trial 24 finished with value: 0.7042408013089672 and parameters: {'binary': False, 'max_features': 5000, 'ngram_range': (1, 1), 'min_df': 4, 'C': 0.11252462573891064}. Best is trial 21 with value: 0.7129523527707503.\n",
      "[I 2025-04-06 22:01:20,758] Trial 25 finished with value: 0.7030589615102266 and parameters: {'binary': False, 'max_features': 5000, 'ngram_range': (1, 1), 'min_df': 3, 'C': 0.022127590508762594}. Best is trial 21 with value: 0.7129523527707503.\n",
      "[I 2025-04-06 22:01:20,822] Trial 26 finished with value: 0.7105142835477313 and parameters: {'binary': False, 'max_features': 5000, 'ngram_range': (1, 1), 'min_df': 3, 'C': 0.052558446154098735}. Best is trial 21 with value: 0.7129523527707503.\n",
      "[I 2025-04-06 22:01:20,887] Trial 27 finished with value: 0.6918519204432276 and parameters: {'binary': False, 'max_features': None, 'ngram_range': (1, 1), 'min_df': 4, 'C': 0.011206648648073119}. Best is trial 21 with value: 0.7129523527707503.\n",
      "[I 2025-04-06 22:01:20,955] Trial 28 finished with value: 0.6973432500946523 and parameters: {'binary': False, 'max_features': 1000, 'ngram_range': (1, 1), 'min_df': 5, 'C': 0.31705575480072173}. Best is trial 21 with value: 0.7129523527707503.\n",
      "[I 2025-04-06 22:01:21,007] Trial 29 finished with value: 0.7107492135224254 and parameters: {'binary': False, 'max_features': None, 'ngram_range': (1, 1), 'min_df': 5, 'C': 0.052976774867961915}. Best is trial 21 with value: 0.7129523527707503.\n",
      "[I 2025-04-06 22:01:21,076] Trial 30 finished with value: 0.6860635627886685 and parameters: {'binary': False, 'max_features': 5000, 'ngram_range': (1, 1), 'min_df': 4, 'C': 0.4454562253576776}. Best is trial 21 with value: 0.7129523527707503.\n",
      "[I 2025-04-06 22:01:21,133] Trial 31 finished with value: 0.7101161804905355 and parameters: {'binary': False, 'max_features': 5000, 'ngram_range': (1, 1), 'min_df': 4, 'C': 0.041082707322863836}. Best is trial 21 with value: 0.7129523527707503.\n",
      "[I 2025-04-06 22:01:21,200] Trial 32 finished with value: 0.7011247890691564 and parameters: {'binary': False, 'max_features': 5000, 'ngram_range': (1, 1), 'min_df': 4, 'C': 0.14427231931947}. Best is trial 21 with value: 0.7129523527707503.\n",
      "[I 2025-04-06 22:01:21,257] Trial 33 finished with value: 0.7041196372502899 and parameters: {'binary': False, 'max_features': 5000, 'ngram_range': (1, 1), 'min_df': 3, 'C': 0.0314327253611493}. Best is trial 21 with value: 0.7129523527707503.\n",
      "[I 2025-04-06 22:01:21,320] Trial 34 finished with value: 0.7046876599274906 and parameters: {'binary': False, 'max_features': 5000, 'ngram_range': (1, 1), 'min_df': 5, 'C': 0.11496066039223793}. Best is trial 21 with value: 0.7129523527707503.\n",
      "[I 2025-04-06 22:01:21,387] Trial 35 finished with value: 0.6717558942743648 and parameters: {'binary': False, 'max_features': 5000, 'ngram_range': (1, 1), 'min_df': 4, 'C': 1.227580586874622}. Best is trial 21 with value: 0.7129523527707503.\n",
      "[I 2025-04-06 22:01:21,441] Trial 36 finished with value: 0.7130317599067598 and parameters: {'binary': False, 'max_features': 3000, 'ngram_range': (1, 1), 'min_df': 4, 'C': 0.054777392960492874}. Best is trial 36 with value: 0.7130317599067598.\n",
      "[I 2025-04-06 22:01:21,565] Trial 37 finished with value: 0.6835040190259867 and parameters: {'binary': False, 'max_features': 3000, 'ngram_range': (1, 2), 'min_df': 5, 'C': 0.20391390000189427}. Best is trial 36 with value: 0.7130317599067598.\n",
      "[I 2025-04-06 22:01:21,617] Trial 38 finished with value: 0.7034483944847137 and parameters: {'binary': False, 'max_features': 3000, 'ngram_range': (1, 1), 'min_df': 3, 'C': 0.016367343833515652}. Best is trial 36 with value: 0.7130317599067598.\n",
      "[I 2025-04-06 22:01:21,729] Trial 39 finished with value: 0.7023256667185791 and parameters: {'binary': False, 'max_features': 3000, 'ngram_range': (1, 2), 'min_df': 4, 'C': 0.05584282989172594}. Best is trial 36 with value: 0.7130317599067598.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "binary: False\n",
      "max_features: 3000\n",
      "ngram_range: (1, 1)\n",
      "min_df: 4\n",
      "C: 0.054777392960492874\n",
      "\n",
      " Validation Set Evaluation:\n",
      "F1 Score: 0.7088866189989785\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.84       838\n",
      "           1       0.78      0.65      0.71       534\n",
      "\n",
      "    accuracy                           0.79      1372\n",
      "   macro avg       0.79      0.77      0.77      1372\n",
      "weighted avg       0.79      0.79      0.79      1372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = df_train[\"text\"]\n",
    "y = df_train[\"target\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    binary = trial.suggest_categorical(\"binary\", [False])\n",
    "    max_features = trial.suggest_categorical(\"max_features\", [1000, 3000, 5000, None])\n",
    "    ngram_range = trial.suggest_categorical(\"ngram_range\", [(1, 1), (1, 2)])\n",
    "    min_df = trial.suggest_int(\"min_df\", 1, 5)\n",
    "\n",
    "    \n",
    "    C = trial.suggest_float(\"C\", 0.01, 10.0, log=True)\n",
    "\n",
    "    vectorizer = CountVectorizer(\n",
    "        binary=binary,\n",
    "        max_features=max_features,\n",
    "        ngram_range=ngram_range,\n",
    "        min_df=min_df\n",
    "    )\n",
    "\n",
    "    model = LinearSVC(C=C, max_iter=10000)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"vectorizer\", vectorizer),\n",
    "        (\"svm\", model)\n",
    "    ])\n",
    "\n",
    "    score = cross_val_score(pipeline, X_train, y_train, scoring=\"f1\", cv=3, n_jobs=-1).mean()\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=40)\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "best_vectorizer = CountVectorizer(\n",
    "    binary=False,\n",
    "    max_features=study.best_params[\"max_features\"],\n",
    "    ngram_range=study.best_params[\"ngram_range\"],\n",
    "    min_df=study.best_params[\"min_df\"]\n",
    ")\n",
    "\n",
    "best_model = LinearSVC(C=study.best_params[\"C\"], max_iter=10000)\n",
    "\n",
    "X_train_final = best_vectorizer.fit_transform(X_train)\n",
    "X_val_final = best_vectorizer.transform(X_val)\n",
    "best_model.fit(X_train_final, y_train)\n",
    "\n",
    "y_pred = best_model.predict(X_val_final)\n",
    "\n",
    "print(\"\\n Validation Set Evaluation:\")\n",
    "print(\"F1 Score:\", f1_score(y_val, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = best_vectorizer.transform(df_test['text'])\n",
    "test_preds = best_model.predict(X_test)\n",
    "\n",
    "df_test[\"target\"] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission_3 = pd.DataFrame()\n",
    "# submission_3['id'] = test_id['id']\n",
    "# submission_3['target'] = test_preds\n",
    "# submission_3.to_csv(\"submission_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:01:21,913] A new study created in memory with name: no-name-4118fde3-bfa3-4b1e-8a5b-24a07ef1f986\n",
      "[I 2025-04-06 22:01:22,038] Trial 0 finished with value: 0.7010822061128826 and parameters: {'max_features': 5000, 'ngram_range': (1, 1), 'min_df': 1, 'C': 3.113342203871947}. Best is trial 0 with value: 0.7010822061128826.\n",
      "[I 2025-04-06 22:01:22,186] Trial 1 finished with value: 0.7074750785422292 and parameters: {'max_features': 5000, 'ngram_range': (1, 2), 'min_df': 2, 'C': 0.061928840289291895}. Best is trial 1 with value: 0.7074750785422292.\n",
      "[I 2025-04-06 22:01:22,243] Trial 2 finished with value: 0.719939214081775 and parameters: {'max_features': None, 'ngram_range': (1, 1), 'min_df': 3, 'C': 0.17632089401186635}. Best is trial 2 with value: 0.719939214081775.\n",
      "[I 2025-04-06 22:01:22,307] Trial 3 finished with value: 0.6795385887049447 and parameters: {'max_features': None, 'ngram_range': (1, 1), 'min_df': 4, 'C': 3.292522761941692}. Best is trial 2 with value: 0.719939214081775.\n",
      "[I 2025-04-06 22:01:22,372] Trial 4 finished with value: 0.6707009516948256 and parameters: {'max_features': 1000, 'ngram_range': (1, 1), 'min_df': 1, 'C': 5.835427758894419}. Best is trial 2 with value: 0.719939214081775.\n",
      "[I 2025-04-06 22:01:22,507] Trial 5 finished with value: 0.7041671752336449 and parameters: {'max_features': 3000, 'ngram_range': (1, 2), 'min_df': 5, 'C': 0.8677783766541183}. Best is trial 2 with value: 0.719939214081775.\n",
      "[I 2025-04-06 22:01:22,619] Trial 6 finished with value: 0.7143880036956541 and parameters: {'max_features': 5000, 'ngram_range': (1, 2), 'min_df': 3, 'C': 0.15977435770419876}. Best is trial 2 with value: 0.719939214081775.\n",
      "[I 2025-04-06 22:01:22,801] Trial 7 finished with value: 0.7266832522785257 and parameters: {'max_features': None, 'ngram_range': (1, 2), 'min_df': 1, 'C': 2.178302505433244}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:22,949] Trial 8 finished with value: 0.7032354104215219 and parameters: {'max_features': None, 'ngram_range': (1, 2), 'min_df': 1, 'C': 0.11359494671058454}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:23,074] Trial 9 finished with value: 0.6921262446786175 and parameters: {'max_features': 1000, 'ngram_range': (1, 2), 'min_df': 4, 'C': 0.9764154549245995}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:23,191] Trial 10 finished with value: 0.6791862215129086 and parameters: {'max_features': 3000, 'ngram_range': (1, 2), 'min_df': 2, 'C': 0.012156758382706098}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:23,257] Trial 11 finished with value: 0.7158567830145119 and parameters: {'max_features': None, 'ngram_range': (1, 1), 'min_df': 3, 'C': 0.49339162711043427}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:23,312] Trial 12 finished with value: 0.7015234594057778 and parameters: {'max_features': None, 'ngram_range': (1, 1), 'min_df': 2, 'C': 0.038206412170324854}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:23,381] Trial 13 finished with value: 0.7115179811321554 and parameters: {'max_features': None, 'ngram_range': (1, 1), 'min_df': 5, 'C': 0.3373648690296792}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:23,485] Trial 14 finished with value: 0.6940026737446138 and parameters: {'max_features': None, 'ngram_range': (1, 2), 'min_df': 4, 'C': 1.4554200121185168}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:23,613] Trial 15 finished with value: 0.681651486406817 and parameters: {'max_features': None, 'ngram_range': (1, 1), 'min_df': 2, 'C': 8.286538725276671}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:23,680] Trial 16 finished with value: 0.7187864908751437 and parameters: {'max_features': None, 'ngram_range': (1, 1), 'min_df': 3, 'C': 0.14573667547273247}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:23,792] Trial 17 finished with value: 0.6791268830701256 and parameters: {'max_features': 1000, 'ngram_range': (1, 2), 'min_df': 1, 'C': 1.9601377071708124}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:23,857] Trial 18 finished with value: 0.7081917327589081 and parameters: {'max_features': 3000, 'ngram_range': (1, 1), 'min_df': 4, 'C': 0.5340637861750597}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:23,951] Trial 19 finished with value: 0.6896964940948241 and parameters: {'max_features': None, 'ngram_range': (1, 2), 'min_df': 2, 'C': 0.023046194967532303}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:24,042] Trial 20 finished with value: 0.7181010922789327 and parameters: {'max_features': None, 'ngram_range': (1, 2), 'min_df': 3, 'C': 0.21523908779689752}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:24,098] Trial 21 finished with value: 0.7147883037464268 and parameters: {'max_features': None, 'ngram_range': (1, 1), 'min_df': 3, 'C': 0.08084959747175346}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:24,167] Trial 22 finished with value: 0.7196363587724298 and parameters: {'max_features': None, 'ngram_range': (1, 1), 'min_df': 3, 'C': 0.19699800465234757}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:24,235] Trial 23 finished with value: 0.7137190735862244 and parameters: {'max_features': None, 'ngram_range': (1, 1), 'min_df': 4, 'C': 0.2956845604901128}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:24,288] Trial 24 finished with value: 0.7083337917824766 and parameters: {'max_features': None, 'ngram_range': (1, 1), 'min_df': 2, 'C': 0.0580147634272475}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:24,357] Trial 25 finished with value: 0.7137616847840128 and parameters: {'max_features': None, 'ngram_range': (1, 1), 'min_df': 3, 'C': 0.6277425861433498}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:24,425] Trial 26 finished with value: 0.7144043286303564 and parameters: {'max_features': 3000, 'ngram_range': (1, 1), 'min_df': 5, 'C': 0.26867250051293984}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:24,490] Trial 27 finished with value: 0.7023582884504501 and parameters: {'max_features': 1000, 'ngram_range': (1, 1), 'min_df': 3, 'C': 0.10413812983213025}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:24,606] Trial 28 finished with value: 0.6947415562636641 and parameters: {'max_features': 5000, 'ngram_range': (1, 2), 'min_df': 2, 'C': 0.03583261442635513}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:24,686] Trial 29 finished with value: 0.7023736554035676 and parameters: {'max_features': 5000, 'ngram_range': (1, 1), 'min_df': 1, 'C': 2.837793415000903}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:24,755] Trial 30 finished with value: 0.7204209578274812 and parameters: {'max_features': None, 'ngram_range': (1, 1), 'min_df': 1, 'C': 1.1836855221787046}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:24,822] Trial 31 finished with value: 0.7178106271402954 and parameters: {'max_features': None, 'ngram_range': (1, 1), 'min_df': 1, 'C': 1.5087422271330517}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:24,915] Trial 32 finished with value: 0.7077971198315822 and parameters: {'max_features': None, 'ngram_range': (1, 1), 'min_df': 1, 'C': 3.87526600231802}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:24,980] Trial 33 finished with value: 0.7150456793029113 and parameters: {'max_features': None, 'ngram_range': (1, 1), 'min_df': 2, 'C': 0.9284981808266978}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:25,049] Trial 34 finished with value: 0.7248315156514176 and parameters: {'max_features': None, 'ngram_range': (1, 1), 'min_df': 1, 'C': 0.4194057785223838}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:25,131] Trial 35 finished with value: 0.7059300498774239 and parameters: {'max_features': 5000, 'ngram_range': (1, 1), 'min_df': 1, 'C': 2.200442835813235}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:25,306] Trial 36 finished with value: 0.7251111718607618 and parameters: {'max_features': None, 'ngram_range': (1, 2), 'min_df': 1, 'C': 5.004194024763036}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:25,509] Trial 37 finished with value: 0.7252252337857646 and parameters: {'max_features': None, 'ngram_range': (1, 2), 'min_df': 1, 'C': 5.523845290223881}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:25,646] Trial 38 finished with value: 0.6471069843597298 and parameters: {'max_features': 1000, 'ngram_range': (1, 2), 'min_df': 1, 'C': 9.934991828703334}. Best is trial 7 with value: 0.7266832522785257.\n",
      "[I 2025-04-06 22:01:25,921] Trial 39 finished with value: 0.7246743002207786 and parameters: {'max_features': None, 'ngram_range': (1, 2), 'min_df': 1, 'C': 4.227008674320192}. Best is trial 7 with value: 0.7266832522785257.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Hyperparameters:\n",
      "max_features: None\n",
      "ngram_range: (1, 2)\n",
      "min_df: 1\n",
      "C: 2.178302505433244\n",
      "\n",
      " Validation Set Evaluation:\n",
      "F1 Score: 0.748062015503876\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85       838\n",
      "           1       0.78      0.72      0.75       534\n",
      "\n",
      "    accuracy                           0.81      1372\n",
      "   macro avg       0.80      0.79      0.80      1372\n",
      "weighted avg       0.81      0.81      0.81      1372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = df_train[\"text\"]\n",
    "y = df_train[\"target\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    max_features = trial.suggest_categorical(\"max_features\", [1000, 3000, 5000, None])\n",
    "    ngram_range = trial.suggest_categorical(\"ngram_range\", [(1, 1), (1, 2)])\n",
    "    min_df = trial.suggest_int(\"min_df\", 1, 5)\n",
    "\n",
    "    C = trial.suggest_float(\"C\", 0.01, 10.0, log=True)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            ngram_range=ngram_range,\n",
    "            min_df=min_df\n",
    "        )),\n",
    "        (\"svm\", CalibratedClassifierCV(LinearSVC(C=C, max_iter=10000), cv=3))\n",
    "    ])\n",
    "\n",
    "    score = cross_val_score(pipeline, X_train, y_train, scoring=\"f1\", cv=3, n_jobs=-1).mean()\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=40)\n",
    "\n",
    "print(\" Best Hyperparameters:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "best_vectorizer = TfidfVectorizer(\n",
    "    max_features=study.best_params[\"max_features\"],\n",
    "    ngram_range=study.best_params[\"ngram_range\"],\n",
    "    min_df=study.best_params[\"min_df\"]\n",
    ")\n",
    "\n",
    "final_svm = CalibratedClassifierCV(\n",
    "    LinearSVC(C=study.best_params[\"C\"], max_iter=10000), cv=3\n",
    ")\n",
    "\n",
    "X_train_vec = best_vectorizer.fit_transform(X_train)\n",
    "X_val_vec = best_vectorizer.transform(X_val)\n",
    "\n",
    "final_svm.fit(X_train_vec, y_train)\n",
    "y_pred = final_svm.predict(X_val_vec)\n",
    "\n",
    "print(\"\\n Validation Set Evaluation:\")\n",
    "print(\"F1 Score:\", f1_score(y_val, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = best_vectorizer.transform(df_test['text'])\n",
    "test_preds = final_svm.predict(X_test)\n",
    "\n",
    "df_test[\"target\"] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission_4 = pd.DataFrame()\n",
    "# submission_4['id'] = test_id['id']\n",
    "# submission_4['target'] = test_preds\n",
    "# submission_4.to_csv(\"submission_4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:01:48,203] A new study created in memory with name: no-name-f3e995e7-32cc-4c2b-9214-e069e7cbd380\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "2025-04-06 22:01:49.913747: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 305us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:01:52,171] Trial 0 finished with value: 0.6996966632962589 and parameters: {'n_layers': 3, 'units_l1': 64, 'units_l2': 64, 'units_l3': 256, 'activation': 'relu', 'dropout_rate': 0.07560367550413033, 'learning_rate': 0.0020560233488107183}. Best is trial 0 with value: 0.6996966632962589.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 265us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:01:53,184] Trial 1 finished with value: 0.7400555041628122 and parameters: {'n_layers': 2, 'units_l1': 64, 'units_l2': 128, 'activation': 'tanh', 'dropout_rate': 0.4538016708524871, 'learning_rate': 0.00725530082003326}. Best is trial 1 with value: 0.7400555041628122.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 462us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:01:54,457] Trial 2 finished with value: 0.7027027027027027 and parameters: {'n_layers': 2, 'units_l1': 128, 'units_l2': 192, 'activation': 'tanh', 'dropout_rate': 0.386467395812036, 'learning_rate': 0.009799367552194675}. Best is trial 1 with value: 0.7400555041628122.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 246us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:01:55,289] Trial 3 finished with value: 0.7467411545623837 and parameters: {'n_layers': 1, 'units_l1': 64, 'activation': 'tanh', 'dropout_rate': 0.40925672394460094, 'learning_rate': 0.0015406183671432944}. Best is trial 3 with value: 0.7467411545623837.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 302us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:01:56,690] Trial 4 finished with value: 0.7418502202643171 and parameters: {'n_layers': 1, 'units_l1': 256, 'activation': 'relu', 'dropout_rate': 0.4776559555562577, 'learning_rate': 0.003100995652822562}. Best is trial 3 with value: 0.7467411545623837.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 324us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:01:58,302] Trial 5 finished with value: 0.7381174277726001 and parameters: {'n_layers': 3, 'units_l1': 256, 'units_l2': 64, 'units_l3': 256, 'activation': 'relu', 'dropout_rate': 0.3224692580040279, 'learning_rate': 0.00955167072840929}. Best is trial 3 with value: 0.7467411545623837.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 253us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:01:59,222] Trial 6 finished with value: 0.7227227227227228 and parameters: {'n_layers': 2, 'units_l1': 64, 'units_l2': 64, 'activation': 'relu', 'dropout_rate': 0.3291977585354237, 'learning_rate': 0.0006274520460955949}. Best is trial 3 with value: 0.7467411545623837.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 271us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:02:00,255] Trial 7 finished with value: 0.7417840375586855 and parameters: {'n_layers': 2, 'units_l1': 64, 'units_l2': 192, 'activation': 'tanh', 'dropout_rate': 0.35132347070160724, 'learning_rate': 0.0070732376180516485}. Best is trial 3 with value: 0.7467411545623837.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 349us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:02:01,791] Trial 8 finished with value: 0.6989748369058714 and parameters: {'n_layers': 3, 'units_l1': 192, 'units_l2': 128, 'units_l3': 192, 'activation': 'tanh', 'dropout_rate': 0.24137286288829096, 'learning_rate': 0.005869476894488713}. Best is trial 3 with value: 0.7467411545623837.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 271us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:02:02,750] Trial 9 finished with value: 0.7456059204440333 and parameters: {'n_layers': 1, 'units_l1': 192, 'activation': 'relu', 'dropout_rate': 0.4641183416315371, 'learning_rate': 0.000460904500349788}. Best is trial 3 with value: 0.7467411545623837.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 255us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:02:03,603] Trial 10 finished with value: 0.7178988326848249 and parameters: {'n_layers': 1, 'units_l1': 128, 'activation': 'tanh', 'dropout_rate': 0.18317916879673304, 'learning_rate': 0.0001103948605289804}. Best is trial 3 with value: 0.7467411545623837.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 276us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:02:04,574] Trial 11 finished with value: 0.7375609756097561 and parameters: {'n_layers': 1, 'units_l1': 192, 'activation': 'relu', 'dropout_rate': 0.49434458102666345, 'learning_rate': 0.0005008206165579386}. Best is trial 3 with value: 0.7467411545623837.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 264us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:02:05,524] Trial 12 finished with value: 0.7360308285163777 and parameters: {'n_layers': 1, 'units_l1': 192, 'activation': 'tanh', 'dropout_rate': 0.4078564216643136, 'learning_rate': 0.0002604701838775023}. Best is trial 3 with value: 0.7467411545623837.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 252us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:02:06,383] Trial 13 finished with value: 0.7497656982193065 and parameters: {'n_layers': 1, 'units_l1': 128, 'activation': 'relu', 'dropout_rate': 0.24462161618328218, 'learning_rate': 0.0014692303107625708}. Best is trial 13 with value: 0.7497656982193065.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 247us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:02:07,232] Trial 14 finished with value: 0.7473583093179635 and parameters: {'n_layers': 1, 'units_l1': 128, 'activation': 'relu', 'dropout_rate': 0.15432598808110748, 'learning_rate': 0.0015702060644501856}. Best is trial 13 with value: 0.7497656982193065.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 245us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:02:08,151] Trial 15 finished with value: 0.7440212577502214 and parameters: {'n_layers': 1, 'units_l1': 128, 'activation': 'relu', 'dropout_rate': 0.10757084889922941, 'learning_rate': 0.0011200954875793775}. Best is trial 13 with value: 0.7497656982193065.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 339us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:02:09,478] Trial 16 finished with value: 0.7207920792079208 and parameters: {'n_layers': 2, 'units_l1': 128, 'units_l2': 256, 'activation': 'relu', 'dropout_rate': 0.16251147464368784, 'learning_rate': 0.002116986338919491}. Best is trial 13 with value: 0.7497656982193065.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 262us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:02:10,338] Trial 17 finished with value: 0.7465034965034965 and parameters: {'n_layers': 1, 'units_l1': 128, 'activation': 'relu', 'dropout_rate': 0.020607512497950203, 'learning_rate': 0.003442408119983431}. Best is trial 13 with value: 0.7497656982193065.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 265us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:02:11,211] Trial 18 finished with value: 0.7475113122171946 and parameters: {'n_layers': 1, 'units_l1': 128, 'activation': 'relu', 'dropout_rate': 0.25218061693386773, 'learning_rate': 0.0008564074383786415}. Best is trial 13 with value: 0.7497656982193065.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 346us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:02:12,630] Trial 19 finished with value: 0.7226061204343535 and parameters: {'n_layers': 2, 'units_l1': 192, 'units_l2': 256, 'activation': 'relu', 'dropout_rate': 0.24961455228416668, 'learning_rate': 0.0008034896507103408}. Best is trial 13 with value: 0.7497656982193065.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 283us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:02:13,859] Trial 20 finished with value: 0.7495361781076066 and parameters: {'n_layers': 2, 'units_l1': 128, 'units_l2': 192, 'activation': 'relu', 'dropout_rate': 0.2892064220601783, 'learning_rate': 0.00033700886783144096}. Best is trial 13 with value: 0.7497656982193065.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 299us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:02:15,057] Trial 21 finished with value: 0.7511916110581506 and parameters: {'n_layers': 2, 'units_l1': 128, 'units_l2': 192, 'activation': 'relu', 'dropout_rate': 0.2842220518133107, 'learning_rate': 0.0002859678214638433}. Best is trial 21 with value: 0.7511916110581506.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 273us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:02:16,302] Trial 22 finished with value: 0.7531956735496559 and parameters: {'n_layers': 2, 'units_l1': 128, 'units_l2': 192, 'activation': 'relu', 'dropout_rate': 0.2811146747068687, 'learning_rate': 0.00024152506562239183}. Best is trial 22 with value: 0.7531956735496559.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 314us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:02:19,134] Trial 23 finished with value: 0.7502392344497608 and parameters: {'n_layers': 3, 'units_l1': 128, 'units_l2': 192, 'units_l3': 64, 'activation': 'relu', 'dropout_rate': 0.20679371019226775, 'learning_rate': 0.0001538114665314072}. Best is trial 22 with value: 0.7531956735496559.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 346us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:02:20,671] Trial 24 finished with value: 0.7208121827411168 and parameters: {'n_layers': 3, 'units_l1': 192, 'units_l2': 192, 'units_l3': 64, 'activation': 'relu', 'dropout_rate': 0.19878922311317826, 'learning_rate': 0.00014794048385653104}. Best is trial 22 with value: 0.7531956735496559.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 315us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:02:22,051] Trial 25 finished with value: 0.7450980392156863 and parameters: {'n_layers': 3, 'units_l1': 128, 'units_l2': 192, 'units_l3': 64, 'activation': 'relu', 'dropout_rate': 0.2892727464473788, 'learning_rate': 0.00022135292572712434}. Best is trial 22 with value: 0.7531956735496559.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 279us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:02:23,111] Trial 26 finished with value: 0.7420289855072464 and parameters: {'n_layers': 2, 'units_l1': 64, 'units_l2': 256, 'activation': 'relu', 'dropout_rate': 0.20815455705625888, 'learning_rate': 0.0001750832076916925}. Best is trial 22 with value: 0.7531956735496559.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 258us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:02:24,190] Trial 27 finished with value: 0.7410281280310378 and parameters: {'n_layers': 2, 'units_l1': 128, 'units_l2': 128, 'activation': 'relu', 'dropout_rate': 0.13306711141340102, 'learning_rate': 0.00010399617382242814}. Best is trial 22 with value: 0.7531956735496559.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 351us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:02:25,955] Trial 28 finished with value: 0.7417923691215617 and parameters: {'n_layers': 3, 'units_l1': 192, 'units_l2': 192, 'units_l3': 128, 'activation': 'relu', 'dropout_rate': 0.2871486526602715, 'learning_rate': 0.0003518290785011598}. Best is trial 22 with value: 0.7531956735496559.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 277us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 22:02:27,122] Trial 29 finished with value: 0.7478591817316841 and parameters: {'n_layers': 3, 'units_l1': 64, 'units_l2': 128, 'units_l3': 128, 'activation': 'relu', 'dropout_rate': 0.06512609212243581, 'learning_rate': 0.00015566019812657855}. Best is trial 22 with value: 0.7531956735496559.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 302us/step\n",
      "\n",
      "Validation F1-score: 0.7396449704142012\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.90      0.85       811\n",
      "           1       0.83      0.67      0.74       561\n",
      "\n",
      "    accuracy                           0.81      1372\n",
      "   macro avg       0.81      0.79      0.79      1372\n",
      "weighted avg       0.81      0.81      0.80      1372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loading of pre-trained Word2Vec\n",
    "print(\"Loading Word2Vec model...\")\n",
    "w2v_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# converting text to mean word embedding\n",
    "def text_to_vector(text, model=w2v_model, dim=300):\n",
    "    words = text.split()\n",
    "    valid_words = [word for word in words if word in model]\n",
    "    if not valid_words:\n",
    "        return np.zeros(dim)\n",
    "    return np.mean([model[word] for word in valid_words], axis=0)\n",
    "\n",
    "\n",
    "X = np.vstack(df_train['text'].apply(text_to_vector))\n",
    "y = df_train['target'].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# MLP model\n",
    "def create_mlp(n_layers, units, dropout_rate, activation, learning_rate):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "    for i in range(n_layers):\n",
    "        model.add(layers.Dense(units[i], activation=activation))\n",
    "        if dropout_rate > 0:\n",
    "            model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    units = [\n",
    "        trial.suggest_int(f\"units_l{i+1}\", 64, 256, step=64) if i < n_layers else 0\n",
    "        for i in range(3)\n",
    "    ]\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"tanh\"])\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5)\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "\n",
    "    model = create_mlp(n_layers, units, dropout_rate, activation, lr)\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "    val_probs = model.predict(X_val)\n",
    "    val_preds = (val_probs >= 0.5).astype(int)\n",
    "    return f1_score(y_val, val_preds)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "\n",
    "best_params = study.best_params\n",
    "units = [best_params.get(\"units_l1\", 0), best_params.get(\"units_l2\", 0), best_params.get(\"units_l3\", 0)]\n",
    "final_model = create_mlp(best_params['n_layers'], units, best_params['dropout_rate'],\n",
    "                         best_params['activation'], best_params['learning_rate'])\n",
    "final_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "val_preds = (final_model.predict(X_val) >= 0.5).astype(int)\n",
    "print(\"\\nValidation F1-score:\", f1_score(y_val, val_preds))\n",
    "print(\"Classification Report:\\n\", classification_report(y_val, val_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 276us/step\n"
     ]
    }
   ],
   "source": [
    "X_test = np.vstack(df_test['text'].apply(text_to_vector))\n",
    "test_preds = (final_model.predict(X_test) >= 0.5).astype(int).ravel()\n",
    "df_test[\"target\"] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission_5 = pd.DataFrame()\n",
    "# submission_5['id'] = test_id['id']\n",
    "# submission_5['target'] = test_preds\n",
    "# submission_5.to_csv(\"submission_5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing again for transformer use only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_transformer(text):\n",
    "    \n",
    "        text = text.strip()                                             # leading/trailing spaces\n",
    "        text = html.unescape(text)                                      # HTML entities\n",
    "        text = text.replace(\"‰ÛÏ\", '\"').replace(\"‰Û\", '\"')            # double quotes\n",
    "        text = text.replace(\"‰Û÷\", \"'\").replace(\"‰Ûª\", \"'\")             # single quotes\n",
    "        text = text.replace(\"‰Û¢\", \"-\").replace(\"‰Û_\", \"-\")             # hyphen \n",
    "        text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)                       # non-ASCII characters\n",
    "        text = re.sub(r'http\\S+', '', text)                             # URLs\n",
    "        text = re.sub(r'@\\w+', '', text)                                # mentions (@username)\n",
    "        text = emoji.demojize(text, delimiters=(\" \", \" \"))              # emojis\n",
    "        text = contractions.fix(text)                                   # contractions\n",
    "        text = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", text)       # urls\n",
    "        text = text.lower()\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Dataset\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "# Test Dataset\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"text\"] = train_df[\"text\"].apply(clean_text_transformer)\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(clean_text_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before drop duplicates 7613\n",
      "after drop duplicates 6949\n"
     ]
    }
   ],
   "source": [
    "print(\"before drop duplicates\", len(train_df))\n",
    "\n",
    "train_df.drop_duplicates(subset=['text'], keep='first', inplace=True)\n",
    "\n",
    "print(\"after drop duplicates\", len(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upgrade transformer just to be on the safe side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvinai/bertweet-base\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_choice, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevice\u001b[49m)\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_choice)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "model_choice = \"vinai/bertweet-base\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_choice, num_labels=2).to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length: 21.11427820832786\n",
      "Median length: 21.0\n",
      "Max length: 86\n",
      "Min length: 3\n"
     ]
    }
   ],
   "source": [
    "tokenized_transformer_lengths = [len(tokenizer.encode(text)) for text in train_df.text]\n",
    "\n",
    "print(\"Mean length:\", np.mean(tokenized_transformer_lengths))\n",
    "print(\"Median length:\", np.median(tokenized_transformer_lengths))\n",
    "print(\"Max length:\", max(tokenized_transformer_lengths))\n",
    "print(\"Min length:\", min(tokenized_transformer_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "train_dataset = train_dataset.rename_column(\"target\", \"labels\")\n",
    "\n",
    "# Split train dataset into 80% train and 20% validation\n",
    "train_val_split = train_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Extract train and validation datasets\n",
    "train_dataset = train_val_split[\"train\"]\n",
    "val_dataset = train_val_split[\"test\"]\n",
    "\n",
    "# Define tokenization function\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_train = train_dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_val = val_dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_test = test_dataset.map(tokenize, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    score = f1_score(labels, predictions, average=\"weighted\")\n",
    "\n",
    "    return {\"f1\": score, \"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "    output_dir=\"./results\",\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    "\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    save_steps=20,\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    load_best_model_at_end=True,\n",
    "\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.05,    \n",
    "    fp16=True  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    compute_metrics = metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(test_dataset=tokenized_test)\n",
    "\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission_8 = pd.DataFrame()\n",
    "# submission_8['id'] = test_id['id']\n",
    "# submission_8['target'] = predicted_labels\n",
    "# submission_8.to_csv(\"submission_transformer_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# files.download('submission_transformer_final.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
